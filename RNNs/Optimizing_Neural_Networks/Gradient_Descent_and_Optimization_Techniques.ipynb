{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent in PyTorch\n",
        "* Notebook by Adam Lang\n",
        "* Date: 9/30/2024\n",
        "\n",
        "# Gradient Descent - Classification\n",
        "* Gradient descent is classified by the amount of data to compute the gradient of loss function.\n",
        "1. Stochastic\n",
        "  * m iterations per epoch.\n",
        "  * SGD Update = 50*10 = 500 times\n",
        "  * Recommendation: shuffle dataset before each epoch!\n",
        "  * SGD will improve model performance by frequently updating weights and biases during each epoch.\n",
        "    * Drawback is that the weights are updated based on just 1 sample which can cause fluctuations in loss values.\n",
        "    * This drawback makes it difficult and rare to use on \"real-world\" data.\n",
        "2. Batch\n",
        "  * Aggregates weights and biases.\n",
        "  * Batch gradient descent update = 10 times for 10 epochs\n",
        "  * **Advantages**:\n",
        "    * computational efficiency\n",
        "    * stable error gradient\n",
        "    * faster convergence\n",
        "  * **Drawbacks**:\n",
        "    * would need several epochs for training\n",
        "    * requires full dataset in memory limiting scalability\n",
        "3. **Mini-Batch**\n",
        "  * **MOST optimal gradient descent method as it combines SGD with Batch.**\n",
        "    * First divides training dataset into subsets.\n",
        "    * During each epoch only update weights ONCE.\n",
        "    * Forward propagation on subset --> loss calculated --> update weights --> next subset, etc..\n",
        "    * Example:\n",
        "      * 50 samples /10 = 5 batch\n",
        "      * num epochs = 5 (weights updated 5 times\n",
        "    * Example 2:\n",
        "      * 800 samples / 80 batch -> 10 subsets\n",
        "      * 10 subsets * 10 epochs = 100 times updated weights\n",
        "\n",
        "* Batch-sizes of 2^n is preferred:\n",
        "   * e.g. 16, 32, 64, 128, 256, 512, 1024, etc..."
      ],
      "metadata": {
        "id": "LRfvJ-R9hfLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Gradient Descent on university data"
      ],
      "metadata": {
        "id": "4p4Ged3QlaYv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cl4-SrZrhW25"
      },
      "outputs": [],
      "source": [
        "## load data\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MOYeCeU_l2lD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## data path\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning Notebooks/Prodigy University Dataset.csv'\n",
        "## load data\n",
        "data = pd.read_csv(data_path)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NruBKQLAo0HF",
        "outputId": "1fd8e9b9-bb12-430a-9abf-9a72119cacac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sat_sum  hs_gpa  fy_gpa\n",
              "0      508    3.40    3.18\n",
              "1      488    4.00    3.33\n",
              "2      464    3.75    3.25\n",
              "3      380    3.75    2.42\n",
              "4      428    4.00    2.63"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-150fe4c4-15ff-41b0-80d0-7393b570060c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sat_sum</th>\n",
              "      <th>hs_gpa</th>\n",
              "      <th>fy_gpa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>508</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>488</td>\n",
              "      <td>4.00</td>\n",
              "      <td>3.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>464</td>\n",
              "      <td>3.75</td>\n",
              "      <td>3.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>380</td>\n",
              "      <td>3.75</td>\n",
              "      <td>2.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>428</td>\n",
              "      <td>4.00</td>\n",
              "      <td>2.63</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-150fe4c4-15ff-41b0-80d0-7393b570060c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-150fe4c4-15ff-41b0-80d0-7393b570060c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-150fe4c4-15ff-41b0-80d0-7393b570060c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-24f6e479-8a77-4b6e-bc78-a4fce4641ac3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-24f6e479-8a77-4b6e-bc78-a4fce4641ac3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-24f6e479-8a77-4b6e-bc78-a4fce4641ac3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"sat_sum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 212,\n        \"max\": 576,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          428,\n          400,\n          408\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hs_gpa\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5416474609669761,\n        \"min\": 1.8,\n        \"max\": 4.5,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          3.3,\n          2.75,\n          3.33\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fy_gpa\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7408051631770596,\n        \"min\": 0.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 282,\n        \"samples\": [\n          3.63,\n          1.78,\n          2.05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing"
      ],
      "metadata": {
        "id": "yyeAGrbll631"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert vars to numpy - 2D array\n",
        "X = data[['sat_sum', 'hs_gpa']].values\n",
        "\n",
        "## reshape fy_gpa into 2D array\n",
        "y = data['fy_gpa'].values.reshape(-1,1)\n",
        "\n",
        "\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60Bg864ul4zm",
        "outputId": "6ef684cd-2493-42ac-a353-db7dade28e75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (1000, 2)\n",
            "Shape of y: (1000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ESx_zVFRmMP7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## standard scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# normalize feature so it is easier to train data\n",
        "## setup scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "## fit_transform X and y train data\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "iq7w6TXIq3m5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## shape X_train\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSZrKz-8mgeQ",
        "outputId": "0dce2f6f-cd49-480e-cbaa-e457f2a9311f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(800, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "## convert numpy to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "t1arDuIfr1ES"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## tensor shape\n",
        "X_train_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKpLcAPTmogf",
        "outputId": "78b6e3fe-eec0-4a27-b45b-36fbee62cff0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([800, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Linear Regression model"
      ],
      "metadata": {
        "id": "f9l-eI4enD6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "CdZ3NAIXnDnS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## build a model with 2 neurons\n",
        "## Sequential --> forward propagation\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 2), ##2 inputs, 2 outputs\n",
        "    nn.Sigmoid(), ## non-linear logistic hidden layer\n",
        "    nn.Linear(2, 1) ##2 inputs, 1 output --> target pred\n",
        ")"
      ],
      "metadata": {
        "id": "i5I_yiC6sqgD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## forward prop\n",
        "preds = model(X_train_tensor)"
      ],
      "metadata": {
        "id": "qAFz8q-knLHA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## first 5 predictions\n",
        "preds[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq5LfohonLE2",
        "outputId": "6b7ffdbb-d092-4c03-a8e6-286e2d8e8f12"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4380],\n",
              "        [-0.4455],\n",
              "        [-0.5019],\n",
              "        [-0.4481],\n",
              "        [-0.5002]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## MSE mean squared error\n",
        "from torch.nn import MSELoss"
      ],
      "metadata": {
        "id": "vh1Zh_1ZnXms"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## compute MSE loss\n",
        "criterion = MSELoss()\n",
        "loss = criterion(preds, y_train_tensor)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcAJlBc-nLCf",
        "outputId": "19843472-ce5c-4192-e527-a1b2a165c3a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9.0811, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## compare reds on X_train vs target\n",
        "preds[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14RRhXNhnK_F",
        "outputId": "fc94bd62-b6d8-4476-bcf0-4f638a4f98c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4380],\n",
              "        [-0.4455],\n",
              "        [-0.5019],\n",
              "        [-0.4481],\n",
              "        [-0.5002]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## y_train\n",
        "y_train_tensor[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8Z-qDuNng9W",
        "outputId": "de458723-f793-47ef-ed98-1ccf8398d76a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.0000],\n",
              "        [3.1100],\n",
              "        [1.6300],\n",
              "        [3.0200],\n",
              "        [1.5500]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Model Weights"
      ],
      "metadata": {
        "id": "e71Cauvhnkds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## model weights\n",
        "model[0].weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsuTIK_2ng6M",
        "outputId": "8f1de37d-fe05-44a0-fce4-ed639e088894"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.3655, -0.0605],\n",
              "        [-0.0774, -0.6032]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## weights\n",
        "model[2].weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBeNVTBrnlxw",
        "outputId": "67446bf3-bc12-4999-9023-99ec284668b2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0879, -0.2757]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "* We will update the model weights using gradient descent to improve our previous linear regression model."
      ],
      "metadata": {
        "id": "eRQnDiX2mvAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "## init optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "jyNbvdgfmsfS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## back propagation\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "1Dza3b4Dm_-E"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## apply update weights to our model\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "FkuNugEjn0th"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## lets see updated weights\n",
        "model[0].weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7Lt9Djfn6bk",
        "outputId": "44324ce3-56a8-4b68-8c33-8a315e4e1a93"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.3655, -0.0605],\n",
              "        [-0.0774, -0.6032]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## weigths\n",
        "model[2].weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx109mNDoAbK",
        "outputId": "57a40db2-544c-4b11-a2b0-8355e2295222"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0852, -0.2736]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "* We can see that SGD updated the weights compared the forward pass."
      ],
      "metadata": {
        "id": "b3WUotrZpPr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manually coding is not efficient to update weights\n",
        "* This is why we can take advantage of the torch.utils.data library to update the weights so we dont have to continually hardcode this."
      ],
      "metadata": {
        "id": "j-L-qaeSpZxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "bTjTsVrpoENA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##setup train data\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)"
      ],
      "metadata": {
        "id": "TY1Ivw94pmQy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## setup our linear regression model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(2, 1)\n",
        ")\n",
        "## optimizer to update weights\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "tltfEHroprHI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## performance on train and test sets before training\n",
        "## criterion is loss function (MSELoss())\n",
        "train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
        "test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
        "print(f\"Without training:\\nTrain Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCxc0D5gp9LO",
        "outputId": "a7f70d03-0bdc-48c0-ec02-7b27161f79ad"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without training:\n",
            "Train Loss: 7.0744, Test Loss: 7.3450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## look at predictions\n",
        "model(X_train_tensor)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm7w6KqQqR8t",
        "outputId": "0d7fdd52-439f-4e0d-c4ae-67eefeb50615"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2516],\n",
              "        [-0.2260],\n",
              "        [-0.0442],\n",
              "        [-0.1580],\n",
              "        [-0.1007]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "* We can see the loss is VERY HIGH and the predictions are all negative.\n",
        "* Let's try varying types of gradient descent to optimize this."
      ],
      "metadata": {
        "id": "8gaMSfS0qnAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "X8WoQKThqwEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## shape of train_data\n",
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-Y7pcLzq9Iu",
        "outputId": "2bdf6dec-59c2-43f2-c267-3703e0618bba"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see there are 800 samples in the dataset.\n",
        "* So based on what we saw above, if we want 10 subsets of 80 --> 800 samples.\n",
        "  * This means we have to update the weights 10 times for 10 epochs = 100 times"
      ],
      "metadata": {
        "id": "zKSILWuGrCdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load data\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=1,\n",
        "                          shuffle=True) ## shuffle the data!\n",
        "\n",
        "## Training loop for 10 epochs\n",
        "for epoch in range(10):\n",
        "  for X_batch, y_batch in train_loader:\n",
        "    # 1. forward pass --> X_batch to model --> get pred\n",
        "    pred = model(X_batch)\n",
        "    # 2. Loss calculate\n",
        "    loss = criterion(pred, y_batch)\n",
        "\n",
        "    # backward pass and optimization\n",
        "    # 3. optimizer zero grad - zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    # 4. loss backwards -- back propagation\n",
        "    loss.backward()\n",
        "    # 5. optimizer step -- add updated weights to modmel\n",
        "    optimizer.step()\n",
        "\n",
        "## apply loss function -- (criterion - MSELoss) -- get train and test loss\n",
        "  train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
        "  # print(epoch,': ',train_loss)\n",
        "  test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
        "  print(f'Epoch {epoch+1}: | Train Loss: {train_loss:.4f}, | Test Loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vepXFxIqhQb",
        "outputId": "81ca792a-dc34-4303-deec-43714d1f4196"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: | Train Loss: 0.5903, | Test Loss: 0.6639\n",
            "Epoch 2: | Train Loss: 0.4793, | Test Loss: 0.5372\n",
            "Epoch 3: | Train Loss: 0.4295, | Test Loss: 0.4874\n",
            "Epoch 4: | Train Loss: 0.3990, | Test Loss: 0.4564\n",
            "Epoch 5: | Train Loss: 0.3801, | Test Loss: 0.4364\n",
            "Epoch 6: | Train Loss: 0.3681, | Test Loss: 0.4283\n",
            "Epoch 7: | Train Loss: 0.3609, | Test Loss: 0.4173\n",
            "Epoch 8: | Train Loss: 0.3555, | Test Loss: 0.4129\n",
            "Epoch 9: | Train Loss: 0.3526, | Test Loss: 0.4099\n",
            "Epoch 10: | Train Loss: 0.3499, | Test Loss: 0.4106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now get predictions\n",
        "model(X_train_tensor)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVoobsB6sYmb",
        "outputId": "fefa0f9d-cffc-49ee-b00a-c786e9337293"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.1802],\n",
              "        [2.1901],\n",
              "        [2.1239],\n",
              "        [2.4677],\n",
              "        [1.9195]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Gradient Descent"
      ],
      "metadata": {
        "id": "69iBbSWAs_5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## linear regression model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(2,1)\n",
        ")\n",
        "## optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "OEpgAfZVs77x"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note below:\n",
        "* We are changing batch_size to 800 to apply entire dataset."
      ],
      "metadata": {
        "id": "60yHxi_gtYXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load data\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=800,\n",
        "                          shuffle=True) ## shuffle the data!\n",
        "\n",
        "## Training loop for 1000 epochs -- more effective training on 800 samples\n",
        "for epoch in range(1000):\n",
        "  for X_batch, y_batch in train_loader:\n",
        "    # 1. forward pass --> X_batch to model --> get pred\n",
        "    pred = model(X_batch)\n",
        "    # 2. Loss calculate\n",
        "    loss = criterion(pred, y_batch)\n",
        "\n",
        "    # backward pass and optimization\n",
        "    # 3. optimizer zero grad - zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    # 4. loss backwards -- back propagation\n",
        "    loss.backward()\n",
        "    # 5. optimizer step -- add updated weights to modmel\n",
        "    optimizer.step()\n",
        "\n",
        "## apply loss function -- (criterion - MSELoss) -- get train and test loss\n",
        "  if (epoch+1) % 100 == 0: ## print after every 100 epochs\n",
        "    train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
        "  # print(epoch,': ',train_loss)\n",
        "    test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
        "    print(f'Epoch {epoch+1}: | Train Loss: {train_loss:.4f}, | Test Loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f2f141-53d6-41d3-f943-1ff4be15d046",
        "id": "5EmtFxqHtRjJ"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: | Train Loss: 0.5743, | Test Loss: 0.6425\n",
            "Epoch 200: | Train Loss: 0.5641, | Test Loss: 0.6296\n",
            "Epoch 300: | Train Loss: 0.5564, | Test Loss: 0.6198\n",
            "Epoch 400: | Train Loss: 0.5502, | Test Loss: 0.6120\n",
            "Epoch 500: | Train Loss: 0.5448, | Test Loss: 0.6054\n",
            "Epoch 600: | Train Loss: 0.5400, | Test Loss: 0.5995\n",
            "Epoch 700: | Train Loss: 0.5354, | Test Loss: 0.5941\n",
            "Epoch 800: | Train Loss: 0.5310, | Test Loss: 0.5891\n",
            "Epoch 900: | Train Loss: 0.5268, | Test Loss: 0.5844\n",
            "Epoch 1000: | Train Loss: 0.5228, | Test Loss: 0.5798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini-Batch Gradient Descent\n",
        "* Now we can take the best of both methods above (SGD + Batch) and get a more optimal outcome."
      ],
      "metadata": {
        "id": "07QqGluFuHyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## linear regression model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(2,1)\n",
        ")\n",
        "## optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "ezIj07HFuQyx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below the batch_size will be 64 a multiple of 2n"
      ],
      "metadata": {
        "id": "IYq6PNN6uVm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load data\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=64, ## 800 samples in train set\n",
        "                          shuffle=True) ## shuffle the data!\n",
        "\n",
        "## Training loop for 500 epochs -- more effective training on 800 samples\n",
        "for epoch in range(500):\n",
        "  for X_batch, y_batch in train_loader:\n",
        "    # 1. forward pass --> X_batch to model --> get pred\n",
        "    pred = model(X_batch)\n",
        "    # 2. Loss calculate\n",
        "    loss = criterion(pred, y_batch)\n",
        "\n",
        "    # backward pass and optimization\n",
        "    # 3. optimizer zero grad - zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    # 4. loss backwards -- back propagation\n",
        "    loss.backward()\n",
        "    # 5. optimizer step -- add updated weights to modmel\n",
        "    optimizer.step()\n",
        "\n",
        "## apply loss function -- (criterion - MSELoss) -- get train and test loss\n",
        "  if (epoch+1) % 100 == 0: ## print after every 100 epochs\n",
        "    train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
        "  # print(epoch,': ',train_loss)\n",
        "    test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
        "    print(f'Epoch {epoch+1}: | Train Loss: {train_loss:.4f}, | Test Loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0ef9a6a-0f9b-44f5-862e-b2a7da464352",
        "id": "M7GZ3H2LuUK4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: | Train Loss: 0.3519, | Test Loss: 0.4066\n",
            "Epoch 200: | Train Loss: 0.3514, | Test Loss: 0.4064\n",
            "Epoch 300: | Train Loss: 0.3509, | Test Loss: 0.4062\n",
            "Epoch 400: | Train Loss: 0.3505, | Test Loss: 0.4059\n",
            "Epoch 500: | Train Loss: 0.3502, | Test Loss: 0.4057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "* Collectively this model was actually not as good as SGD alone, however it was much better than Batch gradient descent alone."
      ],
      "metadata": {
        "id": "rZfkUJCcu4wO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Optimization Techniques\n",
        "* Here are some that we will look at:\n",
        "1. GD with momentum\n",
        "  * slightly modified version of SGD.\n",
        "  * Can prevent the SGD from stopping at the local minima and instead go to the global minima.\n",
        "  * This is most useful for situations where we have both local and global minima.\n",
        "  * **Faster Approach**: moving average of gradients\n",
        "    * Accelerated SGD\n",
        "    * Dampens turbulence\n",
        "  * Uses Exponential Average\n",
        "  * Beta usually kept at 0.9 or 90% weight of previous gradients and 10% weight to current gradient.\n",
        "    * Equivalent to taking average of last 10 gradients.\n",
        "2. Nesterov momentum\n",
        "  * looks ahead at gradient of future steps.\n",
        "3. AdaGrad\n",
        "  * Adaptive Gradient Descent uses different learning rates for each iteration.\n",
        "  * Concept\n",
        "    * params with infrequent updates --> BIGGER updates to weights\n",
        "    * params with frequent updates --> SMALLER updates to weights\n",
        "  * **Very useful for sparse or unstructured datasets.**\n",
        "  * **Problem**: may reduce learning rates aggressively!!\n",
        "    * similar to using a very low learning rate.\n",
        "  * To overcome this problem we can use **Learning Rate Decay**\n",
        "    * used to **reduce the learning rate over time.**\n",
        "    * It can be fixed or scheduled or dynamically adjusted.\n",
        "  * **Default learning rate decay in PyTorch is 0.1**\n",
        "  * **Higher decay rate --> lower new learning rate**\n",
        "4. RMSProp\n",
        "  * Useful for all kinds of datasets.\n",
        "  * \"Root Mean Squared Propagation\" accelerates the optimization process by reducing the number of updates needed to reach the minima.\n",
        "  * Example:\n",
        "    * Build a classification model to classify variety of fishes.\n",
        "    * If the PRIMARY FACTOR is COLOR.\n",
        "    * RMSPROP penalizes the parameter \"Color\" to rely on other features.\n",
        "      * Prevents the model from adapting too quickly to changes in the parameter \"Color\".\n",
        "5. **Adam**\n",
        "  * Adaptive Moment Estimation or Adam.\n",
        "  * Combination of RMSProp and Momentum.\n",
        "  * **MOST WIDELY USED OPTIMIZATION TECHNIQUE IN DEEP LEARNING**\n",
        "  * There are 2 moving averages in Adam.\n",
        "    * 1. first moment (mean) estimate\n",
        "    * 2. second moment (uncentered variance) estimate\n",
        "    * 3. Updated rule for Adam optimizer"
      ],
      "metadata": {
        "id": "unM5wZxevvMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization using Gradient Descent with Momentum\n",
        "* The code difference is adding **momentum** to the optimizer below.\n",
        "* As we mentioned we want it set to 0.9 or 90% to look back at the last 10."
      ],
      "metadata": {
        "id": "D0xRU0PHxHqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## linear regression model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(2,1)\n",
        ")\n",
        "## optimizer --> momentum set at 0.9 now\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "O3C55AvKxN65"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "Z0uYlSYRxUQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load data\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=64, ## 800 samples in train set\n",
        "                          shuffle=True) ## shuffle the data!\n",
        "\n",
        "## Training loop for 500 epochs -- more effective training on 800 samples\n",
        "for epoch in range(500):\n",
        "  for X_batch, y_batch in train_loader:\n",
        "    # 1. forward pass --> X_batch to model --> get pred\n",
        "    pred = model(X_batch)\n",
        "    # 2. Loss calculate\n",
        "    loss = criterion(pred, y_batch)\n",
        "\n",
        "    # backward pass and optimization\n",
        "    # 3. optimizer zero grad - zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    # 4. loss backwards -- back propagation\n",
        "    loss.backward()\n",
        "    # 5. optimizer step -- add updated weights to modmel\n",
        "    optimizer.step()\n",
        "\n",
        "## apply loss function -- (criterion - MSELoss) -- get train and test loss\n",
        "  if (epoch+1) % 100 == 0: ## print after every 100 epochs\n",
        "    train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
        "  # print(epoch,': ',train_loss)\n",
        "    test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
        "    print(f'Epoch {epoch+1}: | Train Loss: {train_loss:.4f}, | Test Loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e98d874-d255-4442-8fec-7fc5f56e56da",
        "id": "g63u7TbRxSN3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: | Train Loss: 0.3464, | Test Loss: 0.4028\n",
            "Epoch 200: | Train Loss: 0.3451, | Test Loss: 0.4023\n",
            "Epoch 300: | Train Loss: 0.3442, | Test Loss: 0.4022\n",
            "Epoch 400: | Train Loss: 0.3434, | Test Loss: 0.4019\n",
            "Epoch 500: | Train Loss: 0.3428, | Test Loss: 0.4017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "* We can see the loss is consistently lower now."
      ],
      "metadata": {
        "id": "vXTUivqtxroH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization using Nesterov Momentum\n",
        "* Here we add **nesterov=True** to the optimizer"
      ],
      "metadata": {
        "id": "hKpFE32UxyL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## linear regression model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(2,1)\n",
        ")\n",
        "## optimizer --> momentum set at 0.9 now\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9,\n",
        "                      nesterov=True)"
      ],
      "metadata": {
        "id": "9y6lpeSzx2iW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "RgZ3Aotrx_Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load data\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=64, ## 800 samples in train set\n",
        "                          shuffle=True) ## shuffle the data!\n",
        "\n",
        "## Training loop for 500 epochs -- more effective training on 800 samples\n",
        "for epoch in range(500):\n",
        "  for X_batch, y_batch in train_loader:\n",
        "    # 1. forward pass --> X_batch to model --> get pred\n",
        "    pred = model(X_batch)\n",
        "    # 2. Loss calculate\n",
        "    loss = criterion(pred, y_batch)\n",
        "\n",
        "    # backward pass and optimization\n",
        "    # 3. optimizer zero grad - zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    # 4. loss backwards -- back propagation\n",
        "    loss.backward()\n",
        "    # 5. optimizer step -- add updated weights to modmel\n",
        "    optimizer.step()\n",
        "\n",
        "## apply loss function -- (criterion - MSELoss) -- get train and test loss\n",
        "  if (epoch+1) % 100 == 0: ## print after every 100 epochs\n",
        "    train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
        "  # print(epoch,': ',train_loss)\n",
        "    test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
        "    print(f'Epoch {epoch+1}: | Train Loss: {train_loss:.4f}, | Test Loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42aca191-0814-4837-a41c-ebf3d88e5b6a",
        "id": "tZJ3-q1Sx-D_"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: | Train Loss: 0.3476, | Test Loss: 0.4032\n",
            "Epoch 200: | Train Loss: 0.3460, | Test Loss: 0.4027\n",
            "Epoch 300: | Train Loss: 0.3449, | Test Loss: 0.4018\n",
            "Epoch 400: | Train Loss: 0.3441, | Test Loss: 0.4016\n",
            "Epoch 500: | Train Loss: 0.3434, | Test Loss: 0.4014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "* Very similar output to GD with momentum but not exactly the same, yet still consistently lower than standard gradient descent techniques.\n",
        "* Lets look at some other optimization techniques."
      ],
      "metadata": {
        "id": "ucYyWv5syIhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization using AdaGrad\n",
        "* params with infrequent updates --> BIGGER updates to weights\n",
        "* params with frequent updates --> SMALLER updates to weights"
      ],
      "metadata": {
        "id": "JAzn6EdfG_Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## linear regression model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(2,1)\n",
        ")\n",
        "## optimizer --> AdaGrad\n",
        "optimizer = optim.Adagrad(model.parameters())"
      ],
      "metadata": {
        "id": "D4FATf2_HD7Y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop\n",
        "* Batch size 64"
      ],
      "metadata": {
        "id": "rAIfZEETHTza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load data\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=64, ## 800 samples in train set\n",
        "                          shuffle=True) ## shuffle the data!\n",
        "\n",
        "## Training loop for 500 epochs -- more effective training on 800 samples\n",
        "for epoch in range(500):\n",
        "  for X_batch, y_batch in train_loader:\n",
        "    # 1. forward pass --> X_batch to model --> get pred\n",
        "    pred = model(X_batch)\n",
        "    # 2. Loss calculate\n",
        "    loss = criterion(pred, y_batch)\n",
        "\n",
        "    # backward pass and optimization\n",
        "    # 3. optimizer zero grad - zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    # 4. loss backwards -- back propagation\n",
        "    loss.backward()\n",
        "    # 5. optimizer step -- add updated weights to modmel\n",
        "    optimizer.step()\n",
        "\n",
        "## apply loss function -- (criterion - MSELoss) -- get train and test loss\n",
        "  if (epoch+1) % 100 == 0: ## print after every 100 epochs\n",
        "    train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
        "  # print(epoch,': ',train_loss)\n",
        "    test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
        "    print(f'Epoch {epoch+1}: | Train Loss: {train_loss:.4f}, | Test Loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d464c055-f2fc-485f-8869-d080ba598573",
        "id": "MYxLrogmHSS1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: | Train Loss: 2.2841, | Test Loss: 2.4530\n",
            "Epoch 200: | Train Loss: 1.2514, | Test Loss: 1.3775\n",
            "Epoch 300: | Train Loss: 0.7755, | Test Loss: 0.8731\n",
            "Epoch 400: | Train Loss: 0.5594, | Test Loss: 0.6395\n",
            "Epoch 500: | Train Loss: 0.4588, | Test Loss: 0.5277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "* We can see the training loss is getting lower!"
      ],
      "metadata": {
        "id": "Uy-n8elSH5ne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization using RMSProp"
      ],
      "metadata": {
        "id": "C7bE2BquIArb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## linear regression model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(2,1)\n",
        ")\n",
        "## optimizer --> RMSprop\n",
        "optimizer = optim.RMSprop(model.parameters())"
      ],
      "metadata": {
        "id": "TkCrFqMQIFJy"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "MgV1cO_7IOtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load data\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=64, ## 800 samples in train set\n",
        "                          shuffle=True) ## shuffle the data!\n",
        "\n",
        "## Training loop for 500 epochs -- more effective training on 800 samples\n",
        "for epoch in range(500):\n",
        "  for X_batch, y_batch in train_loader:\n",
        "    # 1. forward pass --> X_batch to model --> get pred\n",
        "    pred = model(X_batch)\n",
        "    # 2. Loss calculate\n",
        "    loss = criterion(pred, y_batch)\n",
        "\n",
        "    # backward pass and optimization\n",
        "    # 3. optimizer zero grad - zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    # 4. loss backwards -- back propagation\n",
        "    loss.backward()\n",
        "    # 5. optimizer step -- add updated weights to modmel\n",
        "    optimizer.step()\n",
        "\n",
        "## apply loss function -- (criterion - MSELoss) -- get train and test loss\n",
        "  if (epoch+1) % 50 == 0: ## print after every 50 epochs\n",
        "    train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
        "  # print(epoch,': ',train_loss)\n",
        "    test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
        "    print(f'Epoch {epoch+1}: | Train Loss: {train_loss:.4f}, | Test Loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f7568c-6e6f-4e6d-b7b8-1992aa069c9f",
        "id": "fZuW_SACINNa"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50: | Train Loss: 0.3414, | Test Loss: 0.4021\n",
            "Epoch 100: | Train Loss: 0.3402, | Test Loss: 0.3985\n",
            "Epoch 150: | Train Loss: 0.3386, | Test Loss: 0.3990\n",
            "Epoch 200: | Train Loss: 0.3380, | Test Loss: 0.3990\n",
            "Epoch 250: | Train Loss: 0.3379, | Test Loss: 0.3984\n",
            "Epoch 300: | Train Loss: 0.3379, | Test Loss: 0.4016\n",
            "Epoch 350: | Train Loss: 0.3370, | Test Loss: 0.3992\n",
            "Epoch 400: | Train Loss: 0.3395, | Test Loss: 0.3982\n",
            "Epoch 450: | Train Loss: 0.3380, | Test Loss: 0.3976\n",
            "Epoch 500: | Train Loss: 0.3379, | Test Loss: 0.4030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "* We have achieved our lowest loss so far at 0.3379 and 0.4030."
      ],
      "metadata": {
        "id": "nV21gpi8IfrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization using Adam"
      ],
      "metadata": {
        "id": "N7tJwIjrIkiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## linear regression model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(2,1)\n",
        ")\n",
        "## optimizer --> Adam\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "P6Xe7tKqIoxr"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "LW7bVCBLIxBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load data\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=64, ## 800 samples in train set\n",
        "                          shuffle=True) ## shuffle the data!\n",
        "\n",
        "## Training loop for 500 epochs -- more effective training on 800 samples\n",
        "for epoch in range(500):\n",
        "  for X_batch, y_batch in train_loader:\n",
        "    # 1. forward pass --> X_batch to model --> get pred\n",
        "    pred = model(X_batch)\n",
        "    # 2. Loss calculate\n",
        "    loss = criterion(pred, y_batch)\n",
        "\n",
        "    # backward pass and optimization\n",
        "    # 3. optimizer zero grad - zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    # 4. loss backwards -- back propagation\n",
        "    loss.backward()\n",
        "    # 5. optimizer step -- add updated weights to modmel\n",
        "    optimizer.step()\n",
        "\n",
        "## apply loss function -- (criterion - MSELoss) -- get train and test loss\n",
        "  if (epoch+1) % 50 == 0: ## print after every 50 epochs\n",
        "    train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
        "  # print(epoch,': ',train_loss)\n",
        "    test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
        "    print(f'Epoch {epoch+1}: | Train Loss: {train_loss:.4f}, | Test Loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a76d01a-221e-44e6-dd89-161b46d20d79",
        "id": "_A7fnYgmIvhD"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50: | Train Loss: 2.2722, | Test Loss: 2.4410\n",
            "Epoch 100: | Train Loss: 0.5070, | Test Loss: 0.5901\n",
            "Epoch 150: | Train Loss: 0.3666, | Test Loss: 0.4221\n",
            "Epoch 200: | Train Loss: 0.3616, | Test Loss: 0.4137\n",
            "Epoch 250: | Train Loss: 0.3592, | Test Loss: 0.4117\n",
            "Epoch 300: | Train Loss: 0.3565, | Test Loss: 0.4096\n",
            "Epoch 350: | Train Loss: 0.3536, | Test Loss: 0.4072\n",
            "Epoch 400: | Train Loss: 0.3507, | Test Loss: 0.4059\n",
            "Epoch 450: | Train Loss: 0.3483, | Test Loss: 0.4046\n",
            "Epoch 500: | Train Loss: 0.3462, | Test Loss: 0.4030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "* We can see that RMS prop was the best optimizer here but that Adam was a close second and Adam was able to reduce the loss immediately after the first 50 epochs."
      ],
      "metadata": {
        "id": "mcaSdi-lI4Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aLW8O6FLIyfE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}