{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT in HuggingFace - specific model tasks\n",
        "* Notebook by Adam Lang\n",
        "* Date: 6/20/2024\n",
        "* We will demonstrate various use cases of BERT models from huggingface.\n",
        "\n",
        "# Use Cases we will demo with BERT\n",
        "1. Masked token prediction ('fill in the blank')\n",
        "2. Next Sentence Prediction\n",
        "3. Question Answering"
      ],
      "metadata": {
        "id": "In2fnLLP0E9D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50o7UWvkz-hr",
        "outputId": "b7e71338-a3f4-476b-fc65-5fdf6b83fd38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "# install transformers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Tokenizer"
      ],
      "metadata": {
        "id": "iC6l-6sL0Xi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import and instantiate the BERT tokenizer\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "#tokenizer - use bert-base-uncased model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "rccboeHx0Pki"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note about encoding below\n",
        "* The padding parameter was defined with 'max_length' but without a number.\n",
        "* Usually this is a standard number such as 512, 1024, etc.\n",
        "* However, in this case we will default to whatever the max_length that is accepted by the model.\n",
        "* `encode_plus` - this specifically returns a dictionary of values not a list of values.\n",
        "  * The great thing about `encode_plus` is that it returns a lot more enformation such as:\n",
        "    * attention_masks\n",
        "    * input_ids, position ids, etc..\n",
        "  * This allows you to retrieve the specific parts of the encoding."
      ],
      "metadata": {
        "id": "bl1ZHQl118fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define random text for experiment\n",
        "text = 'The Boston Celtics won the 2024 NBA World Championship over the Dallas Mavericks.'\n",
        "\n",
        "# encode text + attention_mask\n",
        "encoding = tokenizer.encode_plus(text, add_special_tokens = True, truncation = True, padding = 'max_length',\n",
        "                                return_attention_mask = True, return_tensors = 'pt')\n",
        "\n"
      ],
      "metadata": {
        "id": "subDeNs_0n-h"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we can print out the encoded text"
      ],
      "metadata": {
        "id": "yFVy6TUX1mOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kwY9VC71jI-",
        "outputId": "e690248f-ce4e-4f86-b3a2-948b28c7b2fa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1996,  3731, 23279,  2180,  1996, 16798,  2549,  6452,  2088,\n",
              "          2528,  2058,  1996,  5759, 28330,  1012,   102,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve specific part of encoding"
      ],
      "metadata": {
        "id": "X9UNgGuJ2z17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need input_ids for this\n",
        "input = encoding['input_ids'][0]\n",
        "attention_mask = encoding['attention_mask'][0]\n",
        "\n",
        "# print attention_mask\n",
        "attention_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN4TCT5-1qEk",
        "outputId": "3a7cd0b4-87ae-41b1-ac47-cee704e021ef"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Mask Tokens (MLM)"
      ],
      "metadata": {
        "id": "IBF27wo030Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.mask_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "j7lwCL9W3GKP",
        "outputId": "a783ca84-16f6-42d6-ef20-4f38aa4f3a5d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[MASK]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Task 1: Predict masked token\n",
        "* MLM is the way to predict words that were originally \"masked\" upon input to the encoder transformer.\n",
        "* Basic concepts: **\"Fill in the blanks\"** --> similar to auto-encoding"
      ],
      "metadata": {
        "id": "Il5KwgQO37a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.nn import functional as F\n",
        "import torch"
      ],
      "metadata": {
        "id": "kvpZjhTb34tt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "AR1aSgOl4Ula"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## BERT model\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict = True) ## to get logits you need return_dict = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqsVLJPD4cX8",
        "outputId": "2cafe06d-ef2d-460c-ba68-0155d5997d9e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "* The core concept of masked language modeling is seen below. We are using the logits or raw prediction probabilities from the transformer to predict the masked tokens or `tokenizer.mask_token`"
      ],
      "metadata": {
        "id": "Bvkscabd5H9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenation with the masked tokens -- the masked token \"fills in the blanks\"\n",
        "text = \"The Opera House in Australia is in, \" +  tokenizer.mask_token + ' city'\n",
        "\n",
        "# invoke tokenizer\n",
        "input = tokenizer.encode_plus(text, return_tensors = 'pt')\n",
        "\n"
      ],
      "metadata": {
        "id": "NPPho08a4tTF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain top 10 predicted masked tokens\n",
        "mask_index = torch.where(input['input_ids'][0] == tokenizer.mask_token_id)"
      ],
      "metadata": {
        "id": "soQi6fLD5EHU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model output\n",
        "output = model(**input)\n",
        "\n",
        "# logits from output (raw probs)\n",
        "logits = output.logits"
      ],
      "metadata": {
        "id": "M3fuXUfw5sTR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## apply Softmax activation function to logits - probabilistic distribution for BERTs vocabulary\n",
        "softmax = F.softmax(logits, dim=-1)"
      ],
      "metadata": {
        "id": "lJH5gj0OLkja"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## mask token index - retrieving top 10 mask tokens\n",
        "\n",
        "mask_word = softmax[0, mask_index, :]"
      ],
      "metadata": {
        "id": "gnp2XPfAL1SO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## top 10 mask tokens\n",
        "top_10 = torch.topk(mask_word, 10, dim=1)[1][0]"
      ],
      "metadata": {
        "id": "jbh6erE8L5zo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## iterate through tensor and replace with mask token\n",
        "for token in top_10:\n",
        "  word = tokenizer.decode([token]) # decode tokens\n",
        "  new_sentence = text.replace(tokenizer.mask_token, word)\n",
        "  print(new_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOlnZL2MMBPB",
        "outputId": "19eb9b0b-18ce-45ef-aafc-ce8b75c01f7f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Opera House in Australia is in, sydney city\n",
            "The Opera House in Australia is in, melbourne city\n",
            "The Opera House in Australia is in, brisbane city\n",
            "The Opera House in Australia is in, adelaide city\n",
            "The Opera House in Australia is in, the city\n",
            "The Opera House in Australia is in, canberra city\n",
            "The Opera House in Australia is in, auckland city\n",
            "The Opera House in Australia is in, hobart city\n",
            "The Opera House in Australia is in, griffith city\n",
            "The Opera House in Australia is in, hume city\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "* The top probabilistic masked token result was Sydney which is correct."
      ],
      "metadata": {
        "id": "Dj_e183KMdK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## if you ONLY wanted the top output not the top 10\n",
        "\n",
        "# activation function\n",
        "softmax = F.softmax(logits, dim=-1)\n",
        "\n",
        "# mask token index\n",
        "mask_word = softmax[0, mask_index, :]\n",
        "\n",
        "top_word = torch.argmax(mask_word, dim=1) #max value is argmax\n",
        "\n",
        "print(tokenizer.decode(top_word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1iNXhnPMQOh",
        "outputId": "d8a7697f-a3b9-462a-dcad-3feb6b56669a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sydney\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Task 2: Next Sentence Prediction\n",
        "* Predicting the next sentence in BERT is the task of predicting whether one sentence follows another.\n",
        "* **Next sentence prediction (NSP):**\n",
        "  * the models concatenate two masked sentences as inputs during pretraining.\n",
        "  * Sometimes they correspond to sentences that were next to each other in the original text, sometimes not.\n",
        "  * The model then has to predict if the two sentences were following each other or not.\n",
        "* huggingface docs: https://huggingface.co/google-bert/bert-base-cased\n",
        "\n",
        "* The model will return:\n",
        "  * logits or `torch.FloatTensor` of shape (`batch_size, 2)\n",
        "  * This is the prediction scoring of the next sequence prediction."
      ],
      "metadata": {
        "id": "sW1JiOWBNaud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## imports\n",
        "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "\n",
        "# tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# model\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "wPKuAjbUM1lp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt and Next sentence\n",
        "* We are going to test whether the next sentence makes sense for the initial prompt."
      ],
      "metadata": {
        "id": "liELiFCZOuRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## test cases - prompt and then the next sentence to predict\n",
        "\n",
        "prompt = \"I drove to the grocery store to get food.\"\n",
        "\n",
        "next_sentence = \"I purchased milk and bread at the store.\""
      ],
      "metadata": {
        "id": "VQbqvt92OXGr"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we are using `encode_plus` which returns a full dictionary."
      ],
      "metadata": {
        "id": "OTMsVHXkO7Zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## encoding text\n",
        "encoding = tokenizer.encode_plus(prompt, next_sentence, return_tensors= 'pt')\n",
        "\n",
        "# outputs\n",
        "outputs = model(**encoding)[0]\n",
        "\n",
        "#activation function\n",
        "softmax = F.softmax(outputs, dim=1)\n",
        "\n",
        "# print softmax\n",
        "print(softmax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asVZZmueOz3M",
        "outputId": "f58da806-15b6-4f90-ebf6-de3b7160ba04"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[9.9999e-01, 7.6648e-06]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "* We obtained the probabilitiies from the logits using the Softmax activation function.\n",
        "* The probabilities are from the prompt and the next sentence.\n",
        "* BERT inserts a SEP token between the sentences.\n",
        "* BERT then outputs a tensor of probabilities from the encoding.\n",
        "* Unlike MLM we are only trying to compute softmax on the 2 inputs.\n",
        "\n",
        "Output interpretation\n",
        "* The closer together the probabilities are the more likely the sentences do not follow one another.\n",
        "* In this case the 2nd probability is smaller than the first so we can say that it is more likely that the 2nd sentence follows the first."
      ],
      "metadata": {
        "id": "VB0GRoCfPSp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Task 3: Question Answering\n",
        "* We can use a specific BERT model for this task: `BertForQuestionAnswering`\n",
        "* We will use this model which was trained cased on the SQUAD dataset: https://huggingface.co/deepset/bert-base-cased-squad2"
      ],
      "metadata": {
        "id": "5D_gv5BbQy_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "#from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
        "\n",
        "# model\n",
        "model = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g__rAoWPRqZ",
        "outputId": "4b08b275-76e3-4e96-91dc-c61d4ad988aa"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question-Answer Task with BERT\n",
        "* We need to give the model example text and an example question."
      ],
      "metadata": {
        "id": "BrCMUKU2TSc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## prompts\n",
        "example_text = \"GPT-3 came out in 2020\"\n",
        "\n",
        "example_question = \"When did GPT-3 come out\""
      ],
      "metadata": {
        "id": "la_lKo-rR3QK"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize inputs\n",
        "tokenized_inputs = tokenizer(example_question, example_text, return_tensors='pt')\n",
        "\n",
        "# print tokenized inputs\n",
        "tokenized_inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpJAw0DiTmUN",
        "outputId": "aa48ea55-dc53-41d4-acd4-9a7569213bef"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1332,  1225, 15175,  1942,   118,   124,  1435,  1149,   102,\n",
              "         15175,  1942,   118,   124,  1338,  1149,  1107, 12795,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of BERT output:\n",
        "* input_ids\n",
        "  * indices of the input sequences in tokenized vocabulary.\n",
        "* token_type_ids\n",
        "  * also called \"segment ids\"\n",
        "    * Represented as a binary mask identifying the 2 types of sequences in our model 0 or 1, which is question and answer (or whatever task you are performing.)\n",
        "  * some models purpose is for classification of pairs of questions such as question-answering.\n",
        "  * `[CLS] SEQUENCE A [SEP] SEQUENCE B [SEP]` --> these are the 2 input sequences, separated by SEP mask.\n",
        "* attention_masks"
      ],
      "metadata": {
        "id": "wUn4B8YhUFw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the BERT question answering model.\n",
        "* we will pass an input passage paragraph followed by a question."
      ],
      "metadata": {
        "id": "tyBXqIgaVMsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The Vatican Apostolic Library more commonly known as the Vatican Library or informally as the Vat, is the library of the Holy See, located in Vatican City, and is the city-state's national library. It was formally established in 1475, although it is much older—it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. The Vatican Library is a research library for history, law, philosophy, science, and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. Pope Nicholas V (1447–1455) envisioned a new Rome, with extensive public works to lure pilgrims and scholars to the city to begin its transformation. Nicolas wanted to create a 'public library' for Rome that was meant to be seen as an institution for humanist scholarship. His death prevented him from carrying out his plan, but his successor Pope Sixtus IV (1471–1484) established what is now known as the Vatican Library. In March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. The Vatican Apostolic Archive was separated from the library at the beginning of the 17th century; it contains another 150,000 items.\"\n",
        "\n",
        "question = \"When was the Vat formally opened?\""
      ],
      "metadata": {
        "id": "3pTtcENvT8C4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
        "\n",
        "# model\n",
        "model = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6dXXz9vWncy",
        "outputId": "3cfd7b9d-afef-4dbb-80d7-306d5dfe489b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## redefine tokenized inputs for new inputs\n",
        "tokenized_inputs = tokenizer(question, text, return_tensors='pt')\n",
        "\n",
        "# context manager - torch\n",
        "with torch.no_grad():\n",
        "  outputs = model(**tokenized_inputs)\n",
        "\n",
        "\n",
        "# answer start logits\n",
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "\n",
        "# predict answer tokens - input_ids from dict\n",
        "predict_answer_tokens = tokenized_inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "\n",
        "# decode to see answer\n",
        "tokenizer.decode(predict_answer_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yzPc3l3SW-8B",
        "outputId": "5f4c4b3f-af3e-4e2c-c161-13be0d542dbd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1475'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "* The model is correct, 1475.\n",
        "* The model was able to use the long context attention mechanism to encode and decode the answer."
      ],
      "metadata": {
        "id": "enoAvFDFX6d8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ur6IdV1lX3AF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}