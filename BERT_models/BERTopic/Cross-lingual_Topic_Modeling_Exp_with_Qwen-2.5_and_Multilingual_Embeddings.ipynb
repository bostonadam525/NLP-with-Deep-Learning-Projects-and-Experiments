{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7015a21-d374-4480-ae7a-8c9e9a277a04",
   "metadata": {},
   "source": [
    "# Cross-lingual Topic Modeling Experiment using Qwen-2.5 and Multilingual Embeddings\n",
    "* Notebook by Adam Lang\n",
    "* Date: 12/17/2024\n",
    "\n",
    "# Overview\n",
    "* This is an experiment performing cross-lingual topic modeling using a multilingual LLM called Qwen-2.5 and multilingual embedding models.\n",
    "* Emphasis on the \"experiment\" as some of the code may still need to be debugged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b3ba7-56ef-4bea-8095-2fd236ed6af2",
   "metadata": {},
   "source": [
    "# Background\n",
    "* 1. Topic modeling is a very common technique used for extracting meaningful insights and modeling data in any industry or domain.\n",
    "    * The topics are used to “tell a story” about a customer’s data through insights and visualizations.\n",
    "\n",
    " \n",
    "* 2. Cross-lingual topic modeling is a common multi-step NLP problem when handling multilingual data.\n",
    "    * Currently the process of working with multilingual data may include the following:\n",
    "         * 1) Language detection → \n",
    "         * 2) Language Translation → \n",
    "         * 3) Topic modeling (e.g. BERTopic) → \n",
    "         * 4) Using an LLM to transform topics into “more interpretable” terminology → \n",
    "         * 5) Creating Data Visualizations to present the Topics to Customers\n",
    "\n",
    "* However, I have developed a technique that leverages various transformer models and a multilingual open source LLM and multilingual embedding model to perform more robust Topic Modeling.\n",
    "\n",
    "* While this process still involves language detection and translation as a pre-processing step, we can leverage the LLM as a checks and balances for the preprocessing language detection and translation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24437b57-78a9-4e59-9819-a47a958a8413",
   "metadata": {},
   "source": [
    "* In addition, there are some robust Data Visualization techniques that we can use to demonstrate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8178eb-889c-4c78-9608-36bcfdcaf054",
   "metadata": {},
   "source": [
    "# Multilingual Data in this dataset\n",
    "* We know from the dataset we have that there are multiple languages represented (however this may not be correct as the data engineering ETL process to detect and classify the languages is not sensitive or specific, howeve this gives me a hint that language detection and translation is an important first preprocessing task before Topic modeling.\n",
    "* Here are some of the known possible languages:\n",
    "1. English (eng)\n",
    "2. Spanish (esl - latin american Spanish)\n",
    "3. Portuguese (por)\n",
    "4. Brazilian (brz)\n",
    "5. French (fra)\n",
    "6. Deutsch (deu)\n",
    "7. French-Canadian (cfr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83fd6d-4c57-41a4-8201-14d0df082071",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "1. Topic Modeling\n",
    "    * We will perform topic modeling using BERTopic and a multilingual LLM as well as extracting keywords using KeyBERT and also MMR (maximal marginal relevance).\n",
    "\n",
    "2. Data Visualization\n",
    "    * We will experiment with some robust data visualization techniques. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357c2c5-b6d5-4a02-b1ca-71bc2c0ba166",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "* Usually, the standard workflow for BERTopic when Topic Modeling is:\n",
    "\n",
    "1. Embedding your documents\n",
    "2. Reducing the dimensionality of embeddings (e.g. UMAP)\n",
    "3. Cluster reduced embeddings (e.g. HDBSCAN)\n",
    "4. Tokenize documents per cluster\n",
    "5. Extract best-representing words per cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ca470-6da4-44cf-ba4c-9130839a0c4a",
   "metadata": {},
   "source": [
    "# Reasoning for using LLMs\n",
    "* While BERTopic and other related models are great for easy analysis and extraction of topics, they often are **\"not interpretable\" and require more human interaction to make them interpretable.**\n",
    "* Quite often BERTopic will give us an output topic such as: \"car-car-store\"\n",
    "    * We could assume this topic is about cars and going to the store but it is very OPEN to interpretation.\n",
    "    * This is where an LLM like LLAMA or Qwen-2.5 can help us produce more interpretable topic names with keywords to better represent the underlying clusters of data.\n",
    "* In particular, I will leverage an open-source LLM called Qwen-2.\n",
    "    * One of Qwen-2's standout features is its **multilingual capabilities**.\n",
    "    * **Thus we can use this model for cross-lingual topic modeling taking advantage of its robust abilities to handle multi-lingual and cross-lingual detection and translation.**\n",
    "    * Qwen-2 has been trained on data spanning an **impressive 27 additional languages**.\n",
    "    * This multilingual training regimen includes languages from diverse regions such as Western Europe, Eastern and Central Europe, the Middle East , Eastern Asia and Southern Asia.\n",
    "    * The main reason we are going to use Qwen-2 is its performance on the MMLU which as a multi-lingual benchmark, it outperforms Llama-3 and Mistral among others in tasks such as the \"Needle in a Haystack Problem.\"\n",
    " \n",
    "# Multilingual Embeddings\n",
    "* I am going to use the LLM for cross-lingual topic modeling, thus as part of the pipeline I need to use embeddings.\n",
    "* You can use ANY embedding model that you want to, here we will use a `SentenceTransformer` model implementation.\n",
    "* However, because we are dealing with multilingual text, I am going to use a multilingual embedding model that has performed well on text clustering and classification tasks which is what we are trying to do.\n",
    " \n",
    "* Embedding Model I am using: `BAAI/bge-m3`\n",
    "* Model card: https://huggingface.co/BAAI/bge-m3\n",
    "* Reasons for using this model:\n",
    "1) Handles long context windows up to 8192 tokens.\n",
    "2) Multilingual support\n",
    "3) Is able to handle 3 common retrieval functionalities of embedding models:\n",
    "    * dense retrieval\n",
    "    * multi-vector retrieval\n",
    "    * sparse retrieval\n",
    " \n",
    "* Future Models to test against this model:\n",
    "  * 1) `jinaai/jina-embeddings-v3`\n",
    "    2) `Snowflake/snowflake-arctic-embed-m`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858f9cf-030d-4d43-b354-624b29171f44",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "* These are the dependencies:\n",
    "\n",
    "1. `bertopic`\n",
    "    * for topic modeling\n",
    "\n",
    "2. `accelerate`\n",
    "    * huggingface library for speeding up and optimizing run-time for LLMs and transformers.\n",
    "\n",
    "3. `bitsandbytes`\n",
    "    * Used for quantization of embeddings and LLMs\n",
    "\n",
    "4. `xformers`\n",
    "    * aims to improve the efficiency and memory usage of Transformers, making it possible to train larger models and handle longer sequences of data.\n",
    "\n",
    "5. `adjustText`\n",
    "    * A library to help you adjust text positions on matplotlib plots to remove or minimize overlaps with each other and data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af114ede-f227-4cf9-9150-947e6f72e72e",
   "metadata": {},
   "source": [
    "# Code needed to work on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14aad9-1239-4d0f-b300-1ce6c2b5e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4164df-de1f-418a-86ab-7e62eafa508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install --upgrade pandas fsspec # sagemaker dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a3d11-aa3e-4d64-84f6-856ebe0976ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture  \n",
    "## upgrade accelerate to use device_map \n",
    "!pip install --upgrade accelerate ## this is for compatability with `bitsandbytes` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4bb1d-6e1d-48d9-b9f7-56756d29034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check accelerate version after upgrade\n",
    "import accelerate\n",
    "print(f\"Accelerate version: {accelerate.__version__}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8518b6-802a-4c3b-8e34-331b3ec31f6b",
   "metadata": {},
   "source": [
    "## Main installations for this experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637d4a5-829e-445d-add9-46f438d6dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install bertopic datasets bitsandbytes xformers adjustText # make sure to install ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3cd6b6-e1d4-4773-b039-7faac03ae972",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "## upgrade torchvision\n",
    "!pip install --upgrade torchvision # if you need to upgrade torchvision run this line\n",
    "!pip install --upgrade torch #upgrade torch version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddbd271-0d90-4036-8df9-18aad0044838",
   "metadata": {},
   "source": [
    "Note: You may need to restart kernel after upgrading both torchvision and torch above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c6e61-0c64-462d-b8a8-835dd26189d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check versions of torch available\n",
    "import torch\n",
    "import torchvision \n",
    "\n",
    "# print versions\n",
    "print(f\"PyTorch version: {torch.__version__}\") \n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e179f0b-24a9-4770-972e-2d32765a90e2",
   "metadata": {},
   "source": [
    "# Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a79b40-242e-4f53-9fa4-e9233f218030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is available \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# set device for PyTorch operations\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.set_device(0) # you can use a different device ID if you have multiple GPUs running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af0a54-b641-4a5a-95b8-189b8fe58292",
   "metadata": {},
   "source": [
    "# Install other Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f19e70-fb44-4b49-b816-ea5992542222",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install seaborn\n",
    "!pip install s3fs #sagemaker dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c534c7-e4fa-40fb-9c3b-cbcfa4128e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard Data Science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import HTML\n",
    "import seaborn as sns\n",
    "\n",
    "## other tools\n",
    "#import response_compare\n",
    "import re\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "import chardet\n",
    "import html #for non ascii detection\n",
    "\n",
    "## transformers and huggingface\n",
    "import transformers\n",
    "#from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import torchvision ## to use Qwen\n",
    "import huggingface_hub\n",
    "\n",
    "## tqdm and pandas\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas(leave=False)\n",
    "#from hashlib import sha256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe3ce6-7a97-4529-8360-4e87dd1122b3",
   "metadata": {},
   "source": [
    "# Load Data from S3 bucket on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50797bf7-0429-49f1-aef5-64ad6073651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "from sagemaker import get_execution_role\n",
    "conn = boto3.client('s3')\n",
    "\n",
    "## load s3 bucket\n",
    "bucket = 'adamnlpbucket1'\n",
    "content = conn.list_objects(Bucket=bucket)['Contents']\n",
    "data_key= '<your S3 bucket file path goes here>'\n",
    "data_location= 's3://{}/{}'.format(bucket,data_key)\n",
    "\n",
    "## load df\n",
    "df = pd.read_csv(data_location, index_col= False, low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c53dcc-967e-4e5f-a7e0-6ec29a1c6d50",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480af00-8482-46e2-a9f4-c17d9d343a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## df info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b859e-6f82-4e24-8283-70f0f38b9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "## df columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b8a41-085d-4d78-b709-47d3d27b1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "## len of df\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a279246-9ef3-405a-9f13-4f62aef39387",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* There are 423,424 rows in the dataframe. We will cut this down to a much smaller sample size for our topic modeling experiment due to time and compute power concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f978a-af55-4871-9a60-51d943f944b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check nulls\n",
    "null_counts = df.isnull().sum().sort_values(ascending=False)\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d110e9-b8ab-4b08-a40a-31568d967abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if null counts in message column\n",
    "df[['message']].isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94184202-4105-4779-8360-db8ea3a226ac",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* I already wrangled this dataset and know that the 56,516 \"null messages\" are not due to any unicode issues but rather due to the message types.\n",
    "* So what I will do is:\n",
    "1. Take a look at the multilingual data first.\n",
    "2. Not remove null values, instead transform them to \"no recognition nmessage\".\n",
    "3. Filter dataframe for a sample size to use in this experiment that includes multilingual data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a519529-ac58-4acd-a3a3-7d5e86f1466c",
   "metadata": {},
   "source": [
    "## Unicode to ascii\n",
    "* Using `unidecode` we will unescape HTML entities and then transliterate non-ASCII characters to their closest ASCII equivalents. Remember to install the unidecode library first with pip install unidecode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c342cd-0f83-4a6d-8b94-0a6c78451e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9573e9-93e7-4583-af78-345a1e35d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['message']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93845e38-a268-4bb6-8c22-8d4f33d77310",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8616c4-a974-444b-842b-ab8e32f46446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "from unidecode import unidecode\n",
    "\n",
    "# function to process text\n",
    "def process_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # First, unescape any HTML entities\n",
    "        unescaped = html.unescape(text)\n",
    "        # Then, transliterate to ASCII\n",
    "        return unidecode(unescaped)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a0b35-01f6-4e25-b98a-41f422811090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to your DataFrame column\n",
    "df['message'] = df['message'].apply(process_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e9bbb-2784-44af-90a7-cc8bb7c44f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check nulls again\n",
    "df[['message']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14d793-866a-4166-8b16-f29f0c77e70e",
   "metadata": {},
   "source": [
    "## Multilingual Data\n",
    "* First lets show the multilingual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd8b4c-0822-4963-bbac-bf38acf72412",
   "metadata": {},
   "outputs": [],
   "source": [
    "## awards received\n",
    "df['rec_language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee55f0-5436-484d-9635-11e0624c958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample of rows with these languages\n",
    "df[['rec_language','message']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe4caf-748f-40fa-8023-991a873e3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "## awards nominated \n",
    "df['nom_language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd2d43-cfce-490e-b79b-b9e0c5d5e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample of rows with these languages\n",
    "df[['nom_language','message']].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59945158-54c6-496a-81ce-504c7cdc0636",
   "metadata": {},
   "source": [
    "# HuggingFace Credentials\n",
    "* This model is open source so no need to apply for credentials.\n",
    "* However, we will input huggingface credentials for downloading models from the huggingface hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf366ff-c102-4da3-b41f-55c015245b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hf hub login\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409cbdc-689e-4252-b289-d0a3c67885dc",
   "metadata": {},
   "source": [
    "# Language Detection \n",
    "* I am not convinced that the languages are correct so I am going to detect them first.\n",
    "* We will use the `xlm-roberta-base-language-detection` transformer trained on 20 languages.\n",
    "* We will get the model from the huggingface hub.\n",
    "* We will use the huggingface pipeline for immediate inference rather than loading the model and the tokenizer we can use it out of the box this way: https://huggingface.co/docs/transformers/main_classes/pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0db2a-ba3b-4f36-b063-f094be24577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model\n",
    "from transformers import pipeline\n",
    "\n",
    "## load model from HF\n",
    "model = pipeline(\n",
    "    'text-classification',\n",
    "    model=\"papluca/xlm-roberta-base-language-detection\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbfec14-ceb1-4d18-b21c-0dce8e6efcd8",
   "metadata": {},
   "source": [
    "## Detect Language\n",
    "* We define a `safe_text function` that converts NaN values to empty strings and ensures all other values are strings.\n",
    "* We apply this function to the 'message' column before converting it to a list.\n",
    "* This ensures that all_text contains only strings, which the model can process.\n",
    "* By handling the NaN values and ensuring all inputs are strings, you should avoid the ValueError you were encountering.\n",
    "\n",
    "Note: For empty strings (which replace NaN values), the model will still make a prediction, but it might not be meaningful. You may want to handle these cases separately in your analysis, perhaps by assigning a special label like \"unknown\" or \"not applicable\" for entries that were originally NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c031432-406c-47f5-b57d-e50266b6860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas() # load tqdm for pandas\n",
    "\n",
    "# Function to safely process text with nan values\n",
    "def safe_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"  # Return empty string for NaN values\n",
    "    return str(text)  # Ensure all non-NaN values are strings\n",
    "\n",
    "# Apply safe_text function to your column with progress bar\n",
    "df['processed_message'] = df['message'].progress_apply(safe_text)\n",
    "\n",
    "# Function to process batches\n",
    "def process_batch(texts):\n",
    "    results = model(texts.tolist(), batch_size=32)\n",
    "    return [d['label'] for d in results]\n",
    "\n",
    "# Process in batches with a progress bar\n",
    "batch_size = 1000  # Adjust this based on memory constraints\n",
    "num_batches = len(df) // batch_size + (1 if len(df) % batch_size != 0 else 0)\n",
    "\n",
    "language_labels = []\n",
    "for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(df))\n",
    "    batch_texts = df['processed_message'].iloc[start_idx:end_idx]\n",
    "    language_labels.extend(process_batch(batch_texts))\n",
    "\n",
    "# Assign labels to DataFrame\n",
    "df['language_label'] = language_labels\n",
    "\n",
    "# Print head\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2132b-404d-4687-863b-0edfb1d4ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724d4d3-8596-447c-8405-f6b88718a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets see the language labels\n",
    "df['language_label'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eca0d6-04c4-4655-8e99-4ee168d19106",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install plotly-express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e0f94-bd5f-435b-b510-60946f399084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px \n",
    "## show languages detected\n",
    "plot_title = \"Languages Detected\"\n",
    "## create labels for x and y axis\n",
    "labels = {\n",
    "    \"x\": \"Language\",\n",
    "    \"y\": \"Count of text\"\n",
    "}\n",
    "## plot histogram of languages for xlm-robeta model\n",
    "fig = px.histogram(df, x=\"language_label\", template=\"plotly_dark\",\n",
    "                   title=plot_title,\n",
    "                   labels=labels)\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c448d8-2ceb-416c-bc66-00000dde8a3d",
   "metadata": {},
   "source": [
    "Languages from `xlm-roberta`\n",
    "\n",
    "```\n",
    "arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c862d8e9-20b3-4a37-a10d-e7369d1911dc",
   "metadata": {},
   "source": [
    "## Non-english text -- check the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0506e4-74fd-45ea-a2dd-896497b7cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets see the Thai outputs since it was #2 on the list\n",
    "thai_rows = df[df['language_label'] == 'th']\n",
    "\n",
    "## print Thai text\n",
    "print('Thai Text identified:')\n",
    "for i, row in thai_rows.iterrows():\n",
    "  print(f\"- {row['message']}\")\n",
    "  if i == 10:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cbd0cb-fc38-4beb-9e52-d3f4617b4d83",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* I don't believe these are Thai, they are probaby just \"not a number\", but we do have to be weary of the issue of certain non-english characters not displaying on screen.\n",
    "* To be sure I am going to utilize some language specific libraries to take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e207156-cf2b-438c-8a2c-6ae162d570d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pythainlp ## Thai specific library in Python: https://pypi.org/project/pythainlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc37719-a705-4620-9973-b2afa85fb2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decode Thai text\n",
    "def decode_thai(text):\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except UnicodeEncodeError:\n",
    "            return text.encode('utf-8-sig').decode('utf-8-sig')\n",
    "    return text\n",
    "\n",
    "# Create a boolean mask for Thai rows\n",
    "thai_mask = df['language_label'] == 'th'\n",
    "\n",
    "# Apply decoding to the original DataFrame using .loc\n",
    "df.loc[thai_mask, 'message'] = df.loc[thai_mask, 'message'].apply(decode_thai)\n",
    "df.loc[thai_mask, 'language_label'] = df.loc[thai_mask, 'language_label'].apply(decode_thai)\n",
    "\n",
    "# Verify the changes\n",
    "print(df.loc[thai_mask, ['message', 'language_label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f9ba8-5dab-4532-acc7-d8669d8e3fa3",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* It appears that the \"Thai\" messages are still showing up as NaN.\n",
    "* Again, these are probably just \"not a number\". And the reason they showed up as Thai is because the word \"Nan\" has 2 meanings in the Thai language --> 1) Grandmother, 2) It is a city north of Bangkok.\n",
    "* Let's try using langdetect instead to see if we are missing any Chinese variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92366fac-71a9-44af-aad5-7a63dc094c8e",
   "metadata": {},
   "source": [
    "### Lang Detect to Extract the Chinese Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49958f0-7df4-4ff1-bb30-3f4fb2aa811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install langdetect #install langdetect library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97222185-026b-432c-a33b-a30d4b400c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "# function to detect chinese language variants\n",
    "def detect_chinese_variant(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        if lang == 'zh-cn':\n",
    "            return 'zh-cn'  # Simplified Chinese\n",
    "        elif lang == 'zh-tw':\n",
    "            return 'zh-tw'  # Traditional Chinese\n",
    "        else:\n",
    "            return 'zh'  # General Chinese\n",
    "    except:\n",
    "        return 'zh'  # If detection fails, return general Chinese\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1da690-0e15-4cc7-849f-fb4183f469f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for Chinese messages\n",
    "chinese_mask = df['language_label'] == 'zh'\n",
    "\n",
    "## get chinese if its there\n",
    "df.loc[chinese_mask, 'language_label'] = df.loc[chinese_mask, 'message'].apply(detect_chinese_variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d349c-6432-4bd0-806d-39d394f40ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['language_label'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa219f4-440f-46d0-8b3f-3f616e8c8df3",
   "metadata": {},
   "source": [
    "Summary\n",
    "* Note, after using langdetect, there was no change in the language detection results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da9c86-b4dc-4f76-b381-0c1f9ef0c0be",
   "metadata": {},
   "source": [
    "## Using Zhon Library to detect Chinese CJK Characters\n",
    "* I am doing this due to validate what was found above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0992c4-a493-4636-a637-b1e9dc9afa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install zhon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5aa133-9a9d-46e1-a29e-f315b4d73240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import re\n",
    "from zhon import hanzi\n",
    "\n",
    "## detect chinese characters using Zhon\n",
    "def is_chinese(text):\n",
    "    if isinstance(text, str):\n",
    "        return bool(re.search(r'[\\u4e00-\\u9fff]+', text))\n",
    "    return False\n",
    "\n",
    "# Create chinese_rows DataFrame\n",
    "chinese_rows = df[df['message'].apply(lambda x: is_chinese(x) if isinstance(x, str) else False)]\n",
    "\n",
    "# Apply the is_chinese function to the 'message' column of chinese_rows\n",
    "chinese_rows['is_chinese'] = chinese_rows['message'].apply(is_chinese)\n",
    "\n",
    "print(chinese_rows['is_chinese'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83770247-0fbb-4321-baca-f4120b5cced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = ['utf-8', 'gb18030', 'gbk', 'gb2312', 'big5']\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        decoded = chinese_rows['message'].str.encode('latin1').str.decode(encoding)\n",
    "        print(f\"Decoding with {encoding}:\")\n",
    "        print(decoded.head())\n",
    "        print(\"---\")\n",
    "    except:\n",
    "        print(f\"Failed to decode with {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c27cf0-1c83-48ec-9a1c-40f847338bb5",
   "metadata": {},
   "source": [
    "### Check for Japanese and Korean\n",
    "* Japanese detection: https://pypi.org/project/pykakasi/\n",
    "* Korean romanization: https://github.com/osori/korean-romanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae75dbc-6c37-426f-96ed-126612bc8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install pykakasi hangul-romanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e8376-c0a4-422f-b83a-b9129d847ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pykakasi hangul-romanize\n",
    "#Now, let's create a function to test for Japanese and Korean:\n",
    "\n",
    "import pykakasi\n",
    "from hangul_romanize import Transliter\n",
    "from hangul_romanize.rule import academic\n",
    "\n",
    "def identify_cjk(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Not a string\"\n",
    "    \n",
    "    # Check for Chinese characters\n",
    "    if any('\\u4e00' <= char <= '\\u9fff' for char in text):\n",
    "        # Try Japanese conversion\n",
    "        kks = pykakasi.kakasi()\n",
    "        result = kks.convert(text)\n",
    "        if any(item['hira'] for item in result):\n",
    "            return \"Likely Japanese\"\n",
    "        else:\n",
    "            return \"Likely Chinese\"\n",
    "    \n",
    "    # Check for Korean characters\n",
    "    elif any('\\uac00' <= char <= '\\ud7a3' for char in text):\n",
    "        transliter = Transliter(academic)\n",
    "        romanized = transliter.romanize(text)\n",
    "        if romanized != text:\n",
    "            return \"Likely Korean\"\n",
    "    \n",
    "    return \"Neither Chinese, Japanese, nor Korean\"\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "df['identified_language'] = df['message'].apply(identify_cjk)\n",
    "\n",
    "# Display results\n",
    "print(df['identified_language'].value_counts())\n",
    "\n",
    "# Show some examples\n",
    "for lang in ['Likely Chinese', 'Likely Japanese', 'Likely Korean']:\n",
    "    print(f\"\\n{lang} examples:\")\n",
    "    print(df[df['identified_language'] == lang]['message'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c37417-6b61-4fc8-9650-d7a60b01fbba",
   "metadata": {},
   "source": [
    "## Additional Language Detection Checks\n",
    "* These checks should give us a much clearer picture of what's in the data that was labeled as Chinese. Based on what we find, we can determine next steps. Some possibilities:\n",
    "\n",
    "* If we're not seeing any Chinese characters at all, the 'zh' label might have been applied incorrectly.\n",
    "* If we're seeing other non-ASCII characters, it might be text in another non-Latin script language.\n",
    "* If we're only seeing ASCII characters, it might be transliterated Chinese or simply mislabeled data.\n",
    "Once we have this information, we can decide how to correctly process and possibly relabel this data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadaf5d6-633c-4cab-9f79-85cc78d49dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cjk(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Not a string\"\n",
    "    if any('\\u4e00' <= char <= '\\u9fff' for char in text):\n",
    "        return \"Contains CJK characters\"\n",
    "    return \"No CJK characters\"\n",
    "\n",
    "df['cjk_check'] = df['message'].apply(check_cjk)\n",
    "print(df['cjk_check'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b0522-a6c0-443e-b8a1-ff34fddc86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_non_ascii(text):\n",
    "    if isinstance(text, str):\n",
    "        return any(ord(char) > 127 for char in text)\n",
    "    return False\n",
    "\n",
    "df['has_non_ascii'] = df['message'].apply(has_non_ascii)\n",
    "print(df['has_non_ascii'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c559657-76b6-4940-9851-974e90068ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_decoding(text):\n",
    "    encodings = ['utf-8', 'iso-8859-1', 'windows-1252', 'ascii']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            return text.encode('iso-8859-1').decode(encoding)\n",
    "        except:\n",
    "            continue\n",
    "    return text\n",
    "\n",
    "df['decoded_message'] = df['message'].apply(lambda x: try_decoding(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0047106-268a-464c-bc27-c82876e4f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['decoded_message'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feed543-5e23-46aa-bb6d-90409a723b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cjk_check_after_decode'] = df['decoded_message'].apply(check_cjk)\n",
    "print(df['cjk_check_after_decode'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c13888-a531-4532-848f-bfc8a2efd798",
   "metadata": {},
   "source": [
    "### Distribution of Language Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe08e1-471b-4a3f-ba80-5a2085d444eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['language_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9b860-a312-4138-b040-1a309923be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_labeled = df[df['language_label'] == 'zh']\n",
    "print(chinese_labeled['message'].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a86a1-a83a-4860-8fbe-41c5caf00b7d",
   "metadata": {},
   "source": [
    "### Let's check the data types in these 'Chinese' labeled rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26bbd5-b900-45fb-9951-54bbd1caf6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chinese_labeled['message'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d0e8c-92ec-4fd2-a5e6-7f446d064be1",
   "metadata": {},
   "source": [
    "### For the string data labeled as Chinese, let's print out some examples along with their byte representation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab8bfa-7554-4909-8b70-677e48534a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_strings = chinese_labeled[chinese_labeled['message'].apply(lambda x: isinstance(x, str))]\n",
    "for idx, row in chinese_strings.head(10).iterrows():\n",
    "    print(f\"Text: {row['message']}\")\n",
    "    print(f\"Bytes: {row['message'].encode('utf-8')}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89e69c-c481-41c8-8b77-251a7a009eb3",
   "metadata": {},
   "source": [
    "### Let's also check for any non-ASCII characters in these 'Chinese' labeled strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108389d3-4b5a-405f-924d-ba502334a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's also check for any non-ASCII characters in these 'Chinese' labeled strings:\n",
    "def has_non_ascii(text):\n",
    "    return any(ord(char) > 127 for char in text)\n",
    "\n",
    "chinese_strings['has_non_ascii'] = chinese_strings['message'].apply(has_non_ascii)\n",
    "print(chinese_strings['has_non_ascii'].value_counts())\n",
    "\n",
    "# Print some examples with non-ASCII characters\n",
    "print(chinese_strings[chinese_strings['has_non_ascii']]['message'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b38db8-6d40-43a2-b2c9-b955a96f38af",
   "metadata": {},
   "source": [
    "### If we're not seeing any Chinese characters, let's check what kind of characters we are seeing in these 'Chinese' labeled rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443830c-4a93-46b3-968e-b00ad787c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def char_details(text):\n",
    "    return [(char, ord(char), unicodedata.name(char, 'Unknown')) for char in text]\n",
    "\n",
    "for idx, row in chinese_strings.head(20).iterrows():\n",
    "    print(f\"Text: {row['message']}\")\n",
    "    print(\"Character details:\")\n",
    "    for char, code, name in char_details(row['message']):\n",
    "        print(f\"  '{char}': U+{code:04X} - {name}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd739d91-c1ec-48cc-829d-c7e4e3097a7c",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* It appears that there is hungarian text mislabeled as Chinese."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d6c2c-814a-4225-91a5-60fbbaecc439",
   "metadata": {},
   "source": [
    "## Correct mislabeled Text from Chinese to Hungarian\n",
    "* We have validated this Chinese text is hungarian and will make sure its fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9a9f5-3b83-49b5-af19-a566d1a63a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "## function to detect unknown\n",
    "def safe_detect(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 'unknown'\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "# Apply language detection to the 'zh' labeled rows\n",
    "chinese_labeled = df[df['language_label'] == 'zh']\n",
    "chinese_labeled['detected_lang'] = chinese_labeled['message'].apply(safe_detect)\n",
    "\n",
    "# Print the distribution of detected languages in 'zh' labeled texts\n",
    "print(\"Distribution of detected languages in 'zh' labeled texts:\")\n",
    "print(chinese_labeled['detected_lang'].value_counts(normalize=True))\n",
    "\n",
    "# Print some examples of texts labeled as 'zh' but detected as Hungarian\n",
    "hungarian_examples = chinese_labeled[chinese_labeled['detected_lang'] == 'hu']\n",
    "print(\"\\nExamples of texts labeled as 'zh' but detected as Hungarian:\")\n",
    "print(hungarian_examples['message'].head(10))\n",
    "\n",
    "# Create a new column for corrected language labels\n",
    "df['corrected_lang'] = df['language_label']\n",
    "\n",
    "# Update the 'zh' labeled rows with the new detection\n",
    "df.loc[df['language_label'] == 'zh', 'corrected_lang'] = df.loc[df['language_label'] == 'zh', 'message'].apply(safe_detect)\n",
    "\n",
    "# Print the distribution of corrected languages\n",
    "print(\"\\nDistribution of corrected languages:\")\n",
    "print(df['corrected_lang'].value_counts())\n",
    "\n",
    "# Check how many 'zh' labels were changed\n",
    "changed_labels = df[df['language_label'] != df['corrected_lang']]\n",
    "print(f\"\\nNumber of changed labels: {len(changed_labels)}\")\n",
    "print(changed_labels['corrected_lang'].value_counts())\n",
    "\n",
    "# Keep both original and corrected labels\n",
    "df['original_lang'] = df['language_label']\n",
    "df['language_label'] = df['corrected_lang']\n",
    "\n",
    "# Print examples of texts that were originally labeled as 'zh' but are now detected as different languages\n",
    "print(\"\\nExamples of texts with changed language labels:\")\n",
    "for lang in df['corrected_lang'].unique():\n",
    "    if lang != 'zh':\n",
    "        print(f\"\\nOriginally labeled as 'zh' but detected as '{lang}':\")\n",
    "        examples = df[(df['original_lang'] == 'zh') & (df['corrected_lang'] == lang)]\n",
    "        print(examples['message'].head(3))\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nFinal language distribution:\")\n",
    "print(df['language_label'].value_counts())\n",
    "\n",
    "print(\"\\nPercentage of 'zh' labels that were changed:\")\n",
    "percent_changed = (len(changed_labels) / len(chinese_labeled)) * 100\n",
    "print(f\"{percent_changed:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6af10-16f2-457e-8ea7-700527de7a11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['message'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64982055-ad40-4dc4-901b-23520a65b535",
   "metadata": {},
   "source": [
    "# Create Sample Data for Topic Modeling\n",
    "* I want to cut down the size of the dataset for this experiment but make sure that we have an adequate representation of the languages in the dataset.\n",
    "* Here is what I did:\n",
    "\n",
    "## Process\n",
    "\n",
    "1. Remove the \"th\" labeled messages as these are really just \"not a number\".\n",
    "2. Remove languages with fewer than 60 samples.\n",
    "3. Keep original sample sizes for languages from \"sw\" down to \"ar\".\n",
    "4. Reduce sample sizes of \"en\", \"es\", and \"pt\" to 2401 (the size of \"sw\" samples), or keeps their original size if it's less than 2401.\n",
    "5. Sample from each language group according to these new sample sizes.\n",
    "6. Combines all samples and shuffles them.\n",
    "\n",
    "Overall, this approach should give a more balanced dataset where the top languages (\"en\", \"es\", \"pt\") have similar representation to \"sw\", while keeping the original sample sizes for less represented languages.\n",
    "\n",
    "We are able to adjust the `min_samples` and `top_language_target` variables if needed. This method ensures that we get a sample that represents each language more equally, with the less common languages maintaining their original representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519310e2-362e-4f96-bb54-e2d158c5c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is the original DataFrame with 'message' and 'language_label' columns\n",
    "\n",
    "# Set the minimum number of samples required for a language to be included\n",
    "min_samples = 60\n",
    "\n",
    "# Set the target size for the top languages (same as 'sw')\n",
    "top_language_target = 2401\n",
    "\n",
    "# Filter out languages with fewer than min_samples and exclude 'th'\n",
    "language_counts = df['language_label'].value_counts()\n",
    "valid_languages = language_counts[(language_counts >= min_samples) & (language_counts.index != 'th')].index\n",
    "\n",
    "# Create a new DataFrame with only the valid languages\n",
    "df_filtered = df[df['language_label'].isin(valid_languages)]\n",
    "\n",
    "# Define the sample sizes for each language\n",
    "language_samples = language_counts[valid_languages].copy()\n",
    "\n",
    "# Set the sample size for top languages to the target\n",
    "top_languages = ['en', 'es', 'pt']\n",
    "for lang in top_languages:\n",
    "    if lang in language_samples.index:\n",
    "        language_samples[lang] = min(top_language_target, language_samples[lang])\n",
    "\n",
    "# Function to sample from each language group\n",
    "def sample_language(group, n):\n",
    "    return group.sample(n=min(len(group), n), replace=False, random_state=42)\n",
    "\n",
    "# Create the final sample\n",
    "final_sample = df_filtered.groupby('language_label').apply(\n",
    "    lambda x: sample_language(x, language_samples[x.name])\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Shuffle the final sample\n",
    "final_sample = final_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Print the distribution in the final sample\n",
    "print(\"Final language distribution:\")\n",
    "print(final_sample['language_label'].value_counts())\n",
    "\n",
    "print(f\"\\nTotal samples: {len(final_sample)}\")\n",
    "\n",
    "# Save the final sample\n",
    "final_sample.to_csv('balanced_language_sample.csv', index=False)\n",
    "print(\"\\nBalanced sample saved to 'balanced_language_sample.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190347b0-af9b-47f7-9301-bb7ff97c8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## final output\n",
    "final_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2386fee-1aab-47e9-935b-8ce381359f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute percentage\n",
    "def perc_of(x,y):\n",
    "    z = round(((len(x)/len(y))*100),3)\n",
    "    return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a74545-67aa-4f78-8953-20043dc6953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## percentage\n",
    "perc_of(final_sample,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab5ea3-1a4e-4540-bf1b-ba7c878662f9",
   "metadata": {},
   "source": [
    "# Qwen-2.5-14B-Instruct\n",
    "* This is the model we will use: https://huggingface.co/Qwen/Qwen2.5-14B-Instruct\n",
    "\n",
    "## Why use an Instruct model?\n",
    "* The key difference between \"Qwen2.5-14B\" and \"Qwen2.5-14B Instruct\" is that the \"Instruct\" version is specifically fine-tuned to better follow instructions and generate text responses that are more aligned with user prompts, making it more suitable for direct interaction through conversational tasks, while the standard \"Qwen2.5-14B\" model serves as a more general-purpose foundation for further customization and fine-tuning by developers.\n",
    "\n",
    "## Why are we using a 14B model and not the largest 72B model?\n",
    "* According to the BERTopic inventor, Maarten says this is a \"nice balance between inference and speed, accuracy and speed.\"\n",
    "* A larger model would be more accurate but compute power would be an issue.\n",
    "* A smaller model would be faster but less accurate.\n",
    "* Thus we will take Maarten's advice and go with the \"in-between\" size.\n",
    "* Previously I have used Llama-2 13B size following this same method and it worked quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80af071-369a-4197-be22-05a1220f5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch -- if not already done above\n",
    "#import torch\n",
    "\n",
    "\n",
    "## model_id\n",
    "model_id = 'Qwen/Qwen2.5-14B-Instruct'\n",
    "\n",
    "## device agnostic code\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4b3ef-1446-49a0-a51c-eca2b9293e53",
   "metadata": {},
   "source": [
    "# LLM Optimization & Quantization\n",
    "* Before we load a 14 BILLION PARAMETER LLM, optimization is a must!\n",
    "* VRAM on any device is limited, so we need to condense the model to run it locally.\n",
    "* There are numerous techniques and tricks to do this but the main principle is 4-bit quantization.\n",
    "* This will reduce the 64-bit representation to only 4-bits which reduces the GPU memory needed to run the LLM model.\n",
    "* More info here:\n",
    "    * QLoRA paper: https://arxiv.org/pdf/2305.14314\n",
    "    * HF blog: https://huggingface.co/blog/4bit-transformers-bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65924e34-b61b-4821-8ac6-f07ad0d69509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "import transformers\n",
    "\n",
    "## now we set the quantization to load LLM with less GPU memory\n",
    "## this requires the `bitsandbytes` library\n",
    "\n",
    "##bits and bytes config -- 32 bit to 4 bit\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True, #4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4', #normalized float 4\n",
    "    bnb_4bit_use_double_quant=True, #second quantization after first\n",
    "    bnb_4bit_compute_dtype=bfloat16 #computation type\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e839c1-6656-4b7c-bade-8f6160c74b68",
   "metadata": {},
   "source": [
    "## 4 Parameters that we use\n",
    "1. **load_in_4bit**\n",
    "    * Allows us to load the model in 4-bit precision compared to the original 32-bit precision. This gives us an incredible speed up and reduces memory!\n",
    "\n",
    "2. **bnb_4bit_quant_type**\n",
    "    * This is the type of 4-bit precision.\n",
    "    * The original paper recommends normalized float 4-bit, so that is what we are going to use!\n",
    "\n",
    "3. **bnb_4bit_use_double_quant**\n",
    "    * This is a neat trick as it performs a second quantization after the first which further reduces the necessary bits\n",
    "\n",
    "4. **bnb_4bit_compute_dtype**\n",
    "    * The compute type used during computation, which further speeds up the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e1374a-3c39-4db5-ad47-de19569beec3",
   "metadata": {},
   "source": [
    "# Load LLM Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb20df-e4c9-4239-8e98-7fde383e14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies from hf\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import accelerate\n",
    "\n",
    "## load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "## Qwen2.5-14B Instruct -- init model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config, #quantization config from above\n",
    "    device_map='auto' ## if you are distriuting across GPUs\n",
    ")\n",
    "\n",
    "## model params\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c301e-fe2d-4396-977d-062d666fee10",
   "metadata": {},
   "source": [
    "# Pipeline Setup\n",
    "* Here we are going to setup a huggingface transformers pipeline for the LLM model.\n",
    "\n",
    "* The specific task we want is text-generation.\n",
    "\n",
    "* Penalty discourages repetitive or redundant output\n",
    "    * It is designed to address the tendency of language models to produce repeated phrases, sentences, or patterns.\n",
    "    * In topic modeling we generally DO NOT want repetitive outputs.\n",
    "    * We also want to set the Temperature lower --> which is more deterministic.\n",
    "\n",
    "* More about pipeline params: https://medium.com/@developer.yasir.pk/understanding-the-controllable-parameters-to-run-inference-your-large-language-model-30643bb46434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a9398-23b9-4299-8857-73a4ac88e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "## text generator pipeline from hugging face\n",
    "generator = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=500,\n",
    "    repetition_penalty=1.1 #penalty discourages redundant output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a98eec0-d50e-4b5a-8afa-2dd702248a24",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "* We can take the model for a \"test drive\" just to test that we are able to prompt the model and get a result.\n",
    "* Then we can go into the specific prompt template we need to instruct the model to perform cross-lingual topic modeling.\n",
    "* Since we are performing cross lingual topic modeling, I am going to ask the model to detect the language of the text I give to it and translate it to english as well as 1 or other languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e77640-bd91-4d1b-82c4-e20ed6d24b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test prompt\n",
    "prompt_test_msg = \"\"\"Laci mar honapok ota vallan viszi a CRM adatmigracios projektet, az elkeszult sztorik leirasa mindig reszletes es atgondolt, a bonyolultabb fejleszteseket/javitasokat pedig a egyutt prezentaljuk a tesztelo kollegaknak, hogy ok is mindig up-to-date-ek legyenek az aktualis valtozasokkal. \n",
    "Lelkiismeretes munkaja peldaerteku mindenki szamara.\"\"\"\n",
    "\n",
    "prompt = f\"Can you detect the language seen in this text: '{prompt_test_msg}', then translate it to english, french, and italian?\"\n",
    "res = generator(prompt)\n",
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb84d7-ace2-4efe-b3c7-aa883a434c96",
   "metadata": {},
   "source": [
    "# Prompt Templates for Qwen-2-14B-Instruct\n",
    "* The prompt templates for this model can be found here:\n",
    "    * Qwen home page: https://qwenlm.github.io/resources/\n",
    "    * Qwen huggingface repo: https://huggingface.co/Qwen/Qwen2.5-14B-Instruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d7485-011d-4981-b720-99176f13e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcaea20-0fb3-4772-9fba-77260fec0bdb",
   "metadata": {},
   "source": [
    "## 1. System Prompt\n",
    "* First prompt needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d5efc-616f-4634-996a-dccdac2f93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompt describes information given to all conversations\n",
    "system_prompt = \"\"\"\n",
    "<human>\n",
    "You are a helpful, respectful, and honest assistant for labeling topics. \n",
    "You are skilled at analyzing text and identifying key themes and concepts.\n",
    "</human>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e498a-3834-42c9-9c76-2b78f6a7043e",
   "metadata": {},
   "source": [
    "## Prompt Template we will use for topic modeling\n",
    "* This is the recommended format for TOPIC MODELING. \n",
    "* It has 2 components:\n",
    "    * a. **example**\n",
    "        * Most LLMs will do much better generating accurate responses if you give them examples to work from. Therefore we need to show an example of what we want our output to be.\n",
    "        * **A note about this prompt. It is better to keep it \"general\" and not specific to your domain or data because I have tried that before and the result creates hallucination issues for the LLM as it thinks it has to focus on the details of your prompt rather than performing topic modeling. Thus, the \"boilerplate\" prompt below which comes from the BERTopic Author Maarten G. is a great example to use and it works well.**\n",
    "      \n",
    "    * b. **main prompt**\n",
    "\n",
    "\n",
    "## 2. example_prompt\n",
    "        * This is the 2nd prompt needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a546687-7f84-4c8b-9b28-b7cc07a672d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt demonstrating the output we are looking for\n",
    "example_prompt = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n",
    "- Meat, but especially beef, is the word food in terms of emissions.\n",
    "- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n",
    "\n",
    "The topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\n",
    "\n",
    "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
    "[/INST] Environmental impacts of eating meat\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb239f8-bc75-4d50-bf32-069ef61380c5",
   "metadata": {},
   "source": [
    "**Summary**:\n",
    "\n",
    "* Give the example documents or text.\n",
    "* Give example keywords.\n",
    "* End by stating \"please create a short label of this topic\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1624bc0-0c8f-4510-8157-a98f10739373",
   "metadata": {},
   "source": [
    "## 3. main_prompt\n",
    "* This is the 3rd prompt needed.\n",
    "* The main prompt contains documents and keywords.\n",
    "* Note about the code below:\n",
    "  * The `[Languages]` placeholder is for the column I created above which is `language_label`. The concept of adding this to the prompt is to feed the LLM the previously detected language as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab508bf9-8351-4b8f-bc56-ab23de658deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## main prompt with documents and keywords\n",
    "main_prompt = \"\"\"\n",
    "<human>\n",
    "Instructions: I have a topic that contains the following documents:\n",
    "[Documents]\n",
    "\n",
    "The topic is described by the following keywords: '[Keywords]'.\n",
    "\n",
    "Based on the information about the topic above, please create a short label for this topic. Make sure to only return the label and nothing more.\n",
    "</human>\n",
    "\n",
    "<response>\n",
    "[Label]\n",
    "</response>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e889f-ebce-4ea6-9520-1867128a2ce1",
   "metadata": {},
   "source": [
    "* The main_prompt has spaces for us to insert what `DOCUMENTS` and `KEYWORDS` we want to use which will change as we perform TOPIC MODELING.\n",
    "\n",
    "* There are two BERTopic-specific tags that are of interest, namely `[DOCUMENTS]` and `[KEYWORDS]`:\n",
    "    * 1. `[DOCUMENTS]` contain the top 5 most relevant documents to the topic.\n",
    "    * 2. `[KEYWORDS]` contain the top 10 most relevant keywords to the topic as generated through c-TF-IDF algorithm from BERTopic.\n",
    "\n",
    "* This template will be completed according to each topic.\n",
    "\n",
    "And finally, we will combine this into our final prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0f68f-cd04-43c4-89f8-eff3935c2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final prompt format\n",
    "full_prompt = system_prompt + example_prompt + main_prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a788266-ffac-4075-9ddf-46e3cfb43405",
   "metadata": {},
   "source": [
    "# Additional Data Preprocessing\n",
    "* Before we create embeddings we want to store the `message` column in a separate variable.\n",
    "\n",
    "Here's a suggested approach:\n",
    "\n",
    "1. Create a new dataframe or a copy of the original dataframe to work with during the embedding creation and topic modeling process. This new dataframe should have the same index as the original dataframe.\n",
    "\n",
    "2. In the new dataframe, create a new column or columns to store the embeddings, leaving the original message column untouched.\n",
    "\n",
    "3. Perform the topic modeling process using the embeddings column(s) in the new dataframe.\n",
    "\n",
    "4. After the topic modeling is complete, create new columns in the new dataframe to store the generated topics, keywords, or any other relevant information.\n",
    "\n",
    "5. Since the new dataframe has the same index as the original dataframe, you can easily merge or join the new dataframe (containing the embeddings, topics, and keywords) back to the original dataframe using the index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1846c-097c-4b53-8b06-63355a2f7a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets review the sample df we created\n",
    "final_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b602f98-9e09-4bfc-9da1-b682406c7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create copy of the sample data\n",
    "df_work = final_sample.copy() \n",
    "df_work.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91702a0-d577-4709-8733-f44ab2e7bc57",
   "metadata": {},
   "source": [
    "# Language Validation and Translation\n",
    "* We can use the LLM to validate the `language_label` that we detected and also translate the `message` column to english prior to topic modeling.\n",
    "* This would take away the need to have the LLM perform BOTH Topic Modeling and Translation all at the same time as it could prevent context window issues having it try and do too much at the same time.\n",
    "* Also, if we can take advantage of the LLM as a seperate language validation and translation machine that would be most ideal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a14322-072b-46cf-ab03-b9a0a2622d5d",
   "metadata": {},
   "source": [
    "## Step 1: Language Validation Using LLM\n",
    "* We will use the LLM to validate the languages detected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d045ccc-cc08-406d-93c7-a8b9440dc831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the LLM to validate language detection done above\n",
    "def validate_language(text, detected_lang):\n",
    "    prompt = f\"\"\"\n",
    "    <human>\n",
    "    Please analyze the following text and determine its language. The previously detected language was {detected_lang}.\n",
    "\n",
    "    Text: \"{text}\"\n",
    "\n",
    "    Instructions:\n",
    "    1. Identify the language of the text.\n",
    "    2. Compare your identification with the previously detected language ({detected_lang}).\n",
    "    3. Return your language identification and whether it matches the previous detection.\n",
    "\n",
    "    Format your response as:\n",
    "    Language: [Your identified language]\n",
    "    Matches Previous Detection: [Yes/No]\n",
    "    </human>\n",
    "\n",
    "    <response>\n",
    "    \"\"\"\n",
    "    \n",
    "    response = generator(prompt)  # Your LLM generation function\n",
    "    return response[0]['generated_text'].strip()\n",
    "\n",
    "# Apply the validation to your dataframe\n",
    "print('Validating language detection....\")\n",
    "tqdm.pandas()\n",
    "df_work['language_validation'] = df_work.progress_apply(lambda row: validate_language(row['message'], row['language_label']), axis=1)\n",
    "\n",
    "## Analyze results\n",
    "# You can then analyze the results to see where the LLM's detection differs from your previous detection\n",
    "mismatches = df_work[df_work['language_validation'].str.contains('Matches Previous Detection: No')]\n",
    "print(f\"Number of language detection mismatches: {len(mismatches)}”)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1954f28b-d023-434e-ab45-def0e4eb51cd",
   "metadata": {},
   "source": [
    "After analyzing and reviewing the results above, you can run the code below to update the language labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf02855-0963-4399-be04-a85c305882b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the language label based on LLM validation\n",
    "df_work['updated_language_label'] = df_work.apply(\n",
    "    lambda row: row['language_validation'].split('\\n')[0].split(': ')[1] \n",
    "    if 'Matches Previous Detection: No' in row['language_validation'] \n",
    "    else row['language_label'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d3f7f3-95f6-4e41-8de3-ccd217e68635",
   "metadata": {},
   "source": [
    "## Step 2: LLM Translation to English\n",
    "* We can now use the LLM Qwen-2.5-14B to translate the text to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c0e38-cd86-43ec-9b51-e2ac4e061c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_english(text, lang):\n",
    "    if lang.lower() == 'english':\n",
    "        return text\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    <human>\n",
    "    Please translate the following text from {lang} to English:\n",
    "\n",
    "    Text: \"{text}\"\n",
    "\n",
    "    Instructions:\n",
    "    1. Provide an accurate English translation of the text.\n",
    "    2. If the text is already in English, simply return \"Already in English\".\n",
    "\n",
    "    Format your response as:\n",
    "    Translation: [English translation or \"Already in English\"]\n",
    "    </human>\n",
    "\n",
    "    <response>\n",
    "    \"\"\"\n",
    "    \n",
    "    response = generator(prompt)  # Your LLM generation function\n",
    "    translation = response[0]['generated_text'].strip()\n",
    "    \n",
    "    # Extract just the translation from the response\n",
    "    if \"Translation:\" in translation:\n",
    "        translation = translation.split(\"Translation:\")[1].strip()\n",
    "    \n",
    "    return translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d33c5-b6e9-4921-b208-5a8c582525e6",
   "metadata": {},
   "source": [
    "After veryifying the code above, run this below to translate the Award Messages to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d23ac2-43b5-4032-ade7-beb10be780ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Translating messages to English...\")\n",
    "df_work['english_translation'] = df_work.progress_apply(\n",
    "    lambda row: translate_to_english(row['message'], row['updated_language_label']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4a4de-c46c-4b17-ad7f-69e29188bd76",
   "metadata": {},
   "source": [
    "## Summary \n",
    "* Now we can use `df_work['english_translation'] for topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb83ddb-1ecd-40ed-a2df-abdb7d6533b9",
   "metadata": {},
   "source": [
    "# BERTopic and Topic Modeling\n",
    "* Now that the LLM is setup we can get into Topic Modeling.\n",
    "\n",
    "## 1. Prepare Embeddings\n",
    "    * Pre-calculation of embeddings for each document to speed-up additional exploration steps and use the embeddings to quickly iterate over BERTopic's hyperparameters as needed.\n",
    "\n",
    "### Multilingual Embeddings\n",
    "* I am going to use the LLM for cross-lingual topic modeling, thus as part of the pipeline I need to use embeddings.\n",
    "* You can use ANY embedding model that you want to, here we will use a `SentenceTransformer implementation of an open source embedding model from Hugging Face. \n",
    "* However, because we are dealing with multilingual text, I am going to use a multilingual embedding model that has performed well on text clustering and classification tasks which is what we are trying to do.\n",
    "* Embedding Model I am using: `BAAI/bge-m3`\n",
    "    * Hugging Face Model card: https://huggingface.co/BAAI/bge-m3\n",
    "    * This model is similar in its functionality to the Jina.ai model that I had originally wanted to use. \n",
    "    * 1) Handles long context windows up to 8192 tokens.\n",
    "    * 2) Multilingual support\n",
    "    * 3) Is able to handle 3 common retrieval functionalities of embedding models:\n",
    "         * dense retrieval\n",
    "         * multi-vector retrieval\n",
    "         * sparse retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5a95a-36ff-4fe6-8af0-dfe5196365be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac30458-e3f0-4c3b-915d-71aa7fe49c9b",
   "metadata": {},
   "source": [
    "### Embedding Optimization\n",
    "* To better optimize the embedding creation process and make better use of compute power I will do the following:\n",
    "    * 1. Use a tqdm progress bar - The tqdm library is used to display a progress bar during the encoding process.\n",
    "    * 2. Use the hugging face accelerate library -\n",
    "        * This automatically handles cuda device placement, distributed training, and mixed precision.\n",
    "        * The Accelerator class is used to prepare the model for acceleration and distribute the computation across multiple devices (if available).\n",
    "\n",
    "    * 3. Use pandas cudflibrary from nvida (I was going to do this but not needed).\n",
    "        * This library helps to optimize the efficiency of pandas dataframe processing.\n",
    "        * It can be used in 2 ways, I will use this method: %load_ext cudf.pandas\n",
    "        * Here is the repo for more information about this very popular library: GitHub - rapidsai/cudf: cuDF - GPU DataFrame Library\n",
    "\n",
    "    * 4. Matryoshka representation (I was going to use this but this is a last resort). \n",
    "\n",
    "\n",
    "#### Embedding Optimization Workflow is as follows: \n",
    "* The workflow is as follows:\n",
    "    * Initialize the Accelerator instance from the accelerate library.\n",
    "    * Load specified sentence transformer model and prepares it for acceleration.\n",
    "    * Iterate over the `message` column of the df_work DataFrame in batches of 64 using the tqdm progress bar.\n",
    "    * Encode each batch of text using `embedding_model.encode` function.\n",
    "    * Gather the results from all processes using `accelerator.gather`.\n",
    "    * Finally, creates a new embeddings column in the `df_work` DataFrame with the computed embeddings.\n",
    "\n",
    "\n",
    "* Overall, this workflow should significantly improve the efficiency of the embedding creation process by utilizing available hardware resources and distributing the computation across multiple devices (if available).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f4758-d52a-4fc5-8e3d-15d30bc5506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check CUDA version before install pandas cudf \n",
    "#!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef4339-4fec-45ea-90d3-d5903f70c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## install version CUDA 12.X - based on cudf github repo: https://github.com/rapidsai/cudf\n",
    "#!pip install --extra-index-url=https://pypi.nvidia.com cudf-cu12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6b714a-ba02-42c6-b653-3e72d6b1f43f",
   "metadata": {},
   "source": [
    "### Create Multilingual Embeddings\n",
    "* The code below will clear the hugging face cache from Transformers library to better optimize memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ef595-362b-408d-8b63-1ab0c2defb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the Hugging Face cache in your current version of the Transformers library:\n",
    "# Import the necessary modules:\n",
    "from transformers import __file__ as transformers_path\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "## Find cache directory path:\n",
    "# Get the path to the Transformers library\n",
    "transformers_dir = Path(transformers_path).parent\n",
    "\n",
    "# Construct the path to the cache directory\n",
    "cache_dir = transformers_dir / \"cached_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fee30-6d08-478e-a208-4c53c72d4ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the cache_dir\n",
    "print(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8acd83-3b58-42b6-a18e-89902a34553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now clear the cache\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d818aa1-80a3-4b89-8ae8-4bee7ba8950a",
   "metadata": {},
   "source": [
    "### Load Embedding Model from HuggingFace\n",
    "* This is the model I am using: https://huggingface.co/BAAI/bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca3c9b-5870-4d34-8ac3-51587de83b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Initialize the accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Set the model name/checkpoint\n",
    "embedding_model_name = \"BAAI/bge-m3\"\n",
    "\n",
    "# Load the model onto the appropriate device\n",
    "embedding_model = SentenceTransformer(embedding_model_name, device=accelerator.device)\n",
    "embedding_model = accelerator.prepare(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894de28-03da-4aaa-afde-10b025bce45f",
   "metadata": {},
   "source": [
    "**Code below creates the actual embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034474c-cb22-4c3b-9608-6642c12f9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings in batches of the message column in the df_work dataframe\n",
    "embeddings = []\n",
    "batch_size = 64\n",
    "bar_format = \"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\"\n",
    "\n",
    "# Create embeddings in batches\n",
    "batches = [df_work['english_translation'].values[i:i + batch_size].tolist() for i in range(0, len(df_work), batch_size)]\n",
    "\n",
    "for batch in tqdm(batches, disable=not accelerator.is_main_process, bar_format=bar_format):\n",
    "    batch_embeddings = embedding_model.encode(batch, show_progress_bar=False, convert_to_tensor=True)\n",
    "    # Ensure the batch_embeddings is 2-dimensional\n",
    "    if batch_embeddings.dim() == 1:\n",
    "        batch_embeddings = batch_embeddings.unsqueeze(0)\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "# Gather the results from all processes\n",
    "embeddings = accelerator.gather(torch.cat(embeddings, dim=0))\n",
    "\n",
    "# Convert the embeddings to a pandas Series\n",
    "if accelerator.is_main_process:\n",
    "    df_work['embeddings'] = pd.Series(embeddings.cpu().numpy().tolist())\n",
    "\n",
    "print(\"Embeddings added to dataframe successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a194ad75-dca4-4384-a528-21501fe9a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "## view embeddings\n",
    "df_work['embeddings'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f303b-64f6-4fb5-b109-4b4d8c0e82d1",
   "metadata": {},
   "source": [
    "## 2. Sub-models\n",
    "* The Next step is to define a few sub-models in BERTopic.\n",
    "* Then we can do some small tweaks to the number of clusters to be created, setting random_states and more.\n",
    "* NOTE: A technique I am NOT using here but is worth trying is hyperparameter tuning using bayesian optimization which would help us choose more refined parameters. I do advise using this technique for larger datasets as every dataset is different in terms of the hyperparameters!\n",
    "\n",
    "\n",
    "The code below will do the following:\n",
    "\n",
    "* Checks format of the embeddings in the DataFrame.\n",
    "* If the embeddings are stored as lists, it converts them to a numpy array.\n",
    "* If they're already numpy arrays, it stacks them into a 2D array.\n",
    "* It then uses this array for the UMAP transformation.\n",
    "* After running this, it will allow us to use reduced_embeddings for further processing or visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da65f55-f15f-4d17-abeb-b2490c796ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# First, let's check the shape of the embeddings and convert them to a numpy array if needed\n",
    "if isinstance(df_work['embeddings'].iloc[0], list):\n",
    "    # If embeddings are stored as lists, convert to numpy array\n",
    "    embeddings_array = np.array(df_work['embeddings'].tolist())\n",
    "else:\n",
    "    # If embeddings are already numpy arrays, just stack them\n",
    "    embeddings_array = np.stack(df_work['embeddings'].values)\n",
    "\n",
    "print(f\"Shape of embeddings array: {embeddings_array.shape}\")\n",
    "\n",
    "# Now Create UMAP model\n",
    "umap_model = UMAP(n_neighbors=15,\n",
    "                  n_components=5,\n",
    "                  min_dist=0.0,\n",
    "                  metric='cosine',\n",
    "                  random_state=42)\n",
    "\n",
    "# Then Create HDBSCAN model\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=150,\n",
    "                        metric='euclidean',\n",
    "                        cluster_selection_method='eom',\n",
    "                        prediction_data=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c07d08f-09e5-4b01-9226-1a36c6eb4f73",
   "metadata": {},
   "source": [
    "### Reduce the embeddings just created to 2 dimensions\n",
    "* This is so we can use them for visualization purposes after creating the topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c346acc-d2f6-428f-b644-b2752d65200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-reduce embeddings for visualization\n",
    "umap_2d = UMAP(n_neighbors=15,\n",
    "               n_components=2,\n",
    "               min_dist=0.0,\n",
    "               metric='cosine',\n",
    "               random_state=42)\n",
    "\n",
    "reduced_embeddings = umap_2d.fit_transform(embeddings_array)\n",
    "\n",
    "## print shape\n",
    "print(f\"Shape of reduced embeddings: {reduced_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad70b0d-c080-4268-b9f0-ae44d78dac56",
   "metadata": {},
   "source": [
    "## 3. Representation Models\n",
    "* One of the ways we are going to represent the topics is with the LLM `Qwen-2.5-14B-Instruct`.\n",
    "    * This should give us a nice set of quality labels for the topics.\n",
    "* However, we might want to have additional representations to view a topic from MULTIPLE angles.\n",
    "* Below we will use the c-TF-IDF from BERTopic as the main representation, and KeyBERT, MMR, and Qwen-2.5-14B-Instruct as the additional representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cdcce9-f274-4d0d-8125-cbf8d157fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import representation models from BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration\n",
    "\n",
    "## 1. KeyBERT\n",
    "keybert = KeyBERTInspired()\n",
    "\n",
    "## 2. MMR\n",
    "mmr = MaximalMarginalRelevance(diversity=0.7) ## higher value --> more diversity keywords\n",
    "\n",
    "## 3. Text Generation with Qwen-2.5-14B-Instruct LLM\n",
    "qwen2 = TextGeneration(generator, prompt=prompt)\n",
    "\n",
    "### ALL Representation models combined\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert,\n",
    "    \"Qwen2\": qwen2,\n",
    "    \"MMR\": mmr,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9f1ad-b1aa-4373-a2e2-6d8da84dae76",
   "metadata": {},
   "source": [
    "# Topic Model Training\n",
    "* Now that ALL models are prepared, we can start training the TOPIC MODEL.\n",
    "* BERTopic is obviously what we are going to use.\n",
    "* We will do the following:\n",
    "    * 1. Give \"sub-models\" to BERTopic.\n",
    "    * 2. Run `.fit_transform`\n",
    "    * 3. Evaluate Topic outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0f9e8-c86c-4502-b837-d8c0bd438af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from tqdm import tqdm\n",
    "\n",
    "# First, let's convert the embeddings to a numpy array\n",
    "if isinstance(df_work['embeddings'].iloc[0], list):\n",
    "    # If embeddings are stored as lists, convert to numpy array\n",
    "    embeddings_array = np.array(df_work['embeddings'].tolist())\n",
    "else:\n",
    "    # If embeddings are already numpy arrays, just stack them\n",
    "    embeddings_array = np.stack(df_work['embeddings'].values)\n",
    "\n",
    "print(f\"Shape of embeddings array: {embeddings_array.shape}\")\n",
    "\n",
    "## setup topic model\n",
    "topic_model = BERTopic(\n",
    "    # sub-models - 4 models\n",
    "    embedding_model=embedding_model, #1\n",
    "    umap_model=umap_model, #2\n",
    "    hdbscan_model=hdbscan_model, #3\n",
    "    representation_model=representation_model, #4\n",
    "\n",
    "    # hyperparameters\n",
    "    top_n_words=10,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Train topic model -- message column & embeddings column\n",
    "topics, probs = tqdm(topic_model.fit_transform(df_work['english_translation'].tolist(), embeddings_array))\n",
    "\n",
    "print(\"Topic modeling complete.\")\n",
    "print(f\"Number of topics found: {len(set(topics)) - 1}\")  # -1 to exclude the -1 topic (outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31b3ad4-070a-486e-b030-8ff14f5bf027",
   "metadata": {},
   "source": [
    "# Topic Model Results Evaluation\n",
    "* Now we can evaluate the results of the topic modmeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077e658-6452-445d-8a02-7ae3cabe8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get topic info -- pandas DF\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c740d-0816-4fc8-bf27-b1ae997929e5",
   "metadata": {},
   "source": [
    "## Deep Dive into a topic\n",
    "* We can now investigate 1 of the topics from the model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049265e9-2238-4025-9a18-ef052cc31783",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic from model --> Keywords extracted using KeyBERT model\n",
    "topic_model.get_topic(0, full=True)['KeyBERT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f77a6-aca0-450d-bb54-339d595628c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic from model --> MMR (maximal marginal relevance)\n",
    "topic_model.get_topic(0, full=True)['MMR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab2e9d-2d0f-43b1-829f-f663325742bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic from model --> Qwen-2 model\n",
    "topic_model.get_topic(0, full=True)['Qwen2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c4df36-0f29-4ddd-8ee9-9e511c1243ac",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "* add info here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415bcf8-fe37-4a9a-8cde-54077d5cd62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## another example of LLM labeled topics\n",
    "topic_model.get_topic(3, full=True)['Qwen2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1b89b-22b5-4dd2-ac34-b287a0e6161d",
   "metadata": {},
   "source": [
    "# Assign Topic Labels using Qwen-2.5-14B-Instruct Model\n",
    "* Now we can use the open source LLM to assign more \"interpretable\" topic labels than what BERTopic would give us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48baeff2-6c05-452c-8121-28825537f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate topic lables using Qwen-2.5-14B-Instruct LLM\n",
    "#qwen2_labels = [label[0][0].split(\"\\n\")[0] for label in topic_model.get_topics(full=True)[\"Qwen2\"].values()]\n",
    "\n",
    "## Tag topic labels to topic_model dataframe\n",
    "#topic_model.set_topic_labels(qwen2_labels)\n",
    "\n",
    "# Function to label topics\n",
    "def get_topic_label(topic_words, topic_docs):\n",
    "    documents = \"\\n\".join(topic_docs[:5])  # Use the first 5 documents as examples\n",
    "    keywords = \", \".join(topic_words)\n",
    "    \n",
    "    prompt = full_prompt.replace(\"[Documents]\", documents).replace(\"[Keywords]\", keywords)\n",
    "    \n",
    "    response = generator(prompt)  # Your LLM generation function\n",
    "    label = response[0]['generated_text'].strip()\n",
    "    return label\n",
    "\n",
    "# Apply to the topics\n",
    "topic_labels = {}\n",
    "for topic_id, topic_info in topic_model.get_topics().items():\n",
    "    if topic_id != -1:  # Exclude the outlier topic\n",
    "        topic_words = [word for word, _ in topic_info]\n",
    "        topic_docs = topic_model.get_representative_docs(topic_id)\n",
    "        label = get_topic_label(topic_words, topic_docs)\n",
    "        topic_labels[topic_id] = label\n",
    "\n",
    "topic_model.set_topic_labels(topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b949f3c-8bac-4785-8510-12dd4f010abf",
   "metadata": {},
   "source": [
    "# Visualize Topics\n",
    "\n",
    "## 1. Interactive Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3470bd-5792-4ffe-8f2d-1106fb38c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic visualization -- interactive\n",
    "topic_model.visualize_documents(titles,\n",
    "                                reduced_embeddings=reduced_embeddings,\n",
    "                                hide_annotations=True,\n",
    "                                hide_document_hover=False,\n",
    "                                custom_labels=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380884b0-62a4-4bf8-ae76-79b8b4c5da30",
   "metadata": {},
   "source": [
    "## 2. Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4e200-6e86-427d-89ce-e7ed8141b9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801aefb-e3cd-4bf4-a53e-58a7b7e158a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625790cd-2fce-46a8-8159-1b321921e906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c507c-348b-4ef3-ba07-c9df29260371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
