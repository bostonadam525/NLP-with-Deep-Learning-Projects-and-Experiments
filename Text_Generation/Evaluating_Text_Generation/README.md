# Evaluating Text Generation
* A repository that contains information on various methods for evaluation of synthetic text generation.


## General Text Generation Evaluation 
* To evaluate the quality of the synthetic notes generated by the LLMs, I often consider the following criteria:
    * **Coherence**: text should be logically structured and flow smoothly from one section to another.
    * **Relevance**: text should focus on the primary concepts and if medical text it should focus on the diagnosis and the specified symptoms, without irrelevant information.
    * **Domain accuracy**: information provided in the text should be domain accurate and follow standard domain practices (e.g. medical, law, media, etc.)
    * **Readability**: text generationed should be easy to read and understand, with clear and concise language.
    * **Completeness**: text should cover all essential sections of a typical domain specific text document (e.g. medical domain it would be hospital history and physical examination, legal it would be a legal brief, etc.)

### Techniques to Measure these criteria
    * 1) **Manual evaluation**
      * Have a panel of domain professionals or subject matter experts review a sample of the generated text and rate them based on the above criteria.
    * 2) **Automated evaluation**
      * Use NLP techniques to evaluate the generated text based on predefined metrics, such as:
        * a. **Coherence**: Measure the semantic similarity between adjacent sentences or sections using embeddings or language models.
          * There is also a technique that leverages the LongFormer Transformer model that can be finetuned to classify text as "coherent" vs. "non-coherent"
            * Model card: https://huggingface.co/lenguist/longformer-coherence-synthetic-classifier
        * b. **Relevance**: Calculate the cosine similarity between the text generated and a given set of entities, terms or queries (e.g. medical domain it would be the  diagnosis/symptom terms). 
        * c. **Domain accuracy**: Use named entity recognition (NER) and relation extraction to identify and validate domain speciic concepts and their relationships within the text. 
        * d. **Readability**: Calculate readability scores (e.g., Flesch-Kincaid, SMOG) to assess the complexity of the language used.
          * There are many readability metrics that exist, most of them are covered in this python library: https://pypi.org/project/py-readability-metrics/
        * e. **Completeness**: Depending on the domain, check for the presence of essential sections of the text (e.g. for medical do main check for chief complaint, history of present illness, review of systems) using text classification models.

### Textual Entailment
* This technique is very commonly used and this is why I have put it within its own category.
* Measuring textual coherence and Generative AI outputs is easy for a human to do (within reason) and more recently has become dominated by using LLM based frameworks to measure if text is coherent and not hallucination.
* “LLM graded evaluations” may help in evaluating text generation to a certain extent, however it is not really a consistent approach.
  * This is because we are using the same LLMs which are by design made to hallucinate outputs.
* This is why we need to look at non-LLM methods of evaluating Generative AI outputs. One such technique is **NLI(Natural Language Inference)**
* Textual entailment recognition is the task of deciding, given two text fragments, whether:
  * The meaning of one text is entailed (can be inferred) from another text (Dagan and Glickman 2004).
  * This task captures generically a broad range of inferences that are relevant for multiple applications.
  * For example, a question answering (QA) system has to identify texts that entail the expected answer. Given the question ‘Who is Kurt Cobain's widow?’ the text ‘There’s a song for Kurt Cobain on nearly every Hole album, spreading Kurt Cobain's influence across the span of Courtney Love’s career. The first is often considered to be ‘Doll Parts’, a track that arrived as the first single to be released from their second album, Live Through This’ which was written about her late husband", entails the expected answer ‘Courtney Love is Kurt Cobain’s widow’.

#### Recognizing textual entailment: Rational, evaluation and approaches
* Book by Dagan et al. 2013: https://link.springer.com/book/10.1007/978-3-031-02151-0
* An NLI model is typically a traditional **multi-classification** deep learning model created through a supervised approach of training with labelled datasets.
* The NLI training or the task formulation follows certain principles to answer: **“Does the premise justify an inference to the hypothesis?”**
* Those principles are as follows (Deb, 2024)[https://medium.com/@rajib76.gcp/nli-natural-language-inferencing-8f231f23800e]
  * 1. Derive the inferential relationship through common sense reason rather that strict logical reasoning
  * 2. Use short and direct inferencing steps, instead of a deductive chain
  * 3. Apply linguistic variation while formulating the tasks

* NLI is thus an approach to determine whether a **hypothesis** (the response)entails(entailment), **contradicts(contradiction)** or is **neutral(neutral)** to a given premise(the context).
* There are various NLI approaches using datasets and models pre-trained and fine-tuned on these tasks:
  * 1. SNLI
       * SNLI or Stanford Natural Language Inference corpus contains around 550K hypothesis/premise pairs. All the premises are image captions from Flickr30K corpus. All the hypothesis were crowd sourced through crowd workers
  * 2. MultiNLI
       * This is based off a famous dataset from NYU: https://huggingface.co/datasets/nyu-mll/multi_nli
       * A model that has been specifically trained on this dataset is the `roberta-large-mnli` from Meta/Facebook: https://huggingface.co/FacebookAI/roberta-large-mnli
  * 3. ANLI
       * ANLI or adversial natural language inference has more than 162K hypothesis/premise pairs. The premises are sourced from diverse sources. The hypothesis were crowd sources and written with the goal of fooling the SOTA models, hence the name adversial.
       * A good example of a model fine-tuned for this task is: `MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli`
         * model card: https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli

* A good list of NLI NLP resources: https://medium.com/@moscantonio/list/nli-nlp-272baac1156a


### Other Techniques to Measure Text Generation
* 1) **CONNER, a COmpreheNsive kNowledge Evaluation fRamework**
      * This is a novel technique proposed by Chen at al. in a 2023 arxiv paper --> link to paper: https://arxiv.org/abs/2310.07289
      * Their technique is designed to systematically and automatically evaluate generated knowledge from 6 important perspectives including:
          * 1. Factuality
          * 2. Relevance
          * 3. Coherence
          * 4. Informativeness
          * 5. Helpfulness
          * 6. Validity

      * Link to github: https://github.com/ChanLiang/CONNER

* 2) **CoUDA: Coherence Evaluation via Unified Data Augmentation**
      * This is a novel technique by Zhu et al. proposed in a 2024 arxiv paper: https://arxiv.org/abs/2404.00681.
      * Due to the paucity of annotated data, data augmentation is commonly used for training coherence evaluation models especially for evaluating synthetic text generation. 
      * The authors state that previous augmentations for this task primarily rely on heuristic rules, lacking designing criteria as guidance. 
      * In this paper, the authors propose a data augmentation framework entitled "COUDA".
        * The technique breaks down discourse coherence into global and local aspects, and designs augmentation strategies for both aspects, respectively. 
        * For local coherence they propose a novel generative strategy for constructing augmentation samples, which involves post-pretraining a generative model and applying two controlling mechanisms to control the difficulty of generated samples. 
          * During inference, COUDA also jointly evaluates both global and local aspects to comprehensively assess the overall coherence of a discourse. 
          * The authors conducted extensive experiments in coherence evaluation to show that with only 233M parameters, COUDA achieves state-ofthe-art performance in both pointwise scoring and pairwise ranking tasks, even surpassing recent GPT-3.5 and GPT-4 based metrics.
         
* 3) **Shuffle Test**
     * Laban et al. in a 2021 paper critically examined the usefulness of the "Shuffle Test" to measure text coherence and developed a novel technique to overcome its limitations called the "K-Block Shuffle Test". 
       * arxiv paper link: https://arxiv.org/abs/2107.03448
       * github link: https://github.com/tingofurro/shuffle_test
     * The Shuffle Test was first introduced by Barzilay and Lapata in 2008.
         * It is the most common task for coherence model evaluation.
         * The task is a binary classification problem, in which a model must discriminate between a document and a shuffled document, obtained by randomly shuffling the order of sentences in the document. The most common dataset for evaluation is a set of articles from the Wall Street Journal, but obviously this is adopted based on the specific domain of interest.
     * The Insertion Test is when a single sentence from a document is removed, and the model must predict the sentence position.
         * Typically, models assign a score to each possible position, and predict the one with highest score.
         * A major limitation of the Insertion Test is that model accuracies are low, often in the 10-20% range (Elsner and Charniak, 2011).
      
     * The Sentence Ordering Task is when a model is given an randomly ordered sentence set, and must produce the correct ordering of sentences.
         * This task is often restricted to generative models, as it is prohibitively expensive to score all combinations to extract a best-scoring order (Logeswaran et al., 2018).
    * The authors introduce the novel **K-block Shuffle Test** to improve upon the limitations of the Shuffle test and its iterations.
        * In the standard Shuffle Test, text is divided into sentences and shuffled, with a unit of one sentence.
        * However, in the k-Block Shuffle Test, sentences are grouped in contiguous blocks of k sentences (resembling paragraphs), and the blocks are shuffled, maintaining sentence order in each block.
        * Within a block, sentences remain locally coherent, and as block size increases, the fraction of correct sentence transitions increases, while potentially incoherent transitions decrease.


  




## Medical Domain
1. Medcat
   * While this is not a direct evaluation tool, it is often used in tandem with text generation models.
   * Link to github: https://github.com/CogStack/MedCAT
  
2. medAlpaca: Finetuned Large Language Models for Medical Question Answering
   * These are an advanced suite of large language models specifically fine-tuned for medical question-answering and dialogue applications.
   * While the main intention of these models is for buiding medical chatbots, you could extend use cases to evaluating medical text generation.
   * Link to repo: https://github.com/kbressem/medAlpaca
  
3. RaTEScore
   * This is a novel, entity-aware metric to assess the quality of medical reports generated by AI models.
   * It emphasizes common and significant medical entities such as diagnostic outcomes and anatomical details, and is robust against complex medical synonyms and sensitive to negation expressions.
   * The evaluations demonstrate that RaTEScore aligns more closely with human preference than existing metrics.
   * While the primary use case is for Radiology Report Generation Evaluation, the concepts can be fine-tuned and extended to other medical report use cases.
   * The authors have released a research paper that explains some of the major issues with current text generation evaluation metrics, see the authors links here:
     * Original 2024 arxiv paper here: https://arxiv.org/abs/2406.16845
     * Authors research homepage: https://angelakeke.github.io/RaTEScore/
     * RaTEScore github: https://github.com/MAGIC-AI4Med/RaTEScore
     * An excellent Figure from the original paper from Zhao et al. showing the limitations of current metrics for evaluation of medical text generation:
       ![image](https://github.com/user-attachments/assets/39ba7de7-a7a7-4460-b306-9d2eeccc6e31)


4. PMC-LLaMA: Towards Building Open-source Language Models for Medicine
   * Original paper by Wu et al. 2023: https://arxiv.org/abs/2304.14454
   * PMC-LLaMA github: https://github.com/chaoyi-wu/PMC-LLaMA
  
5. LongHealth: A Question Answering Benchmark with Long Clinical Documents
   * LongHealth is a comprehensive benchmark designed to evaluate the capabilities of large language models (LLMs) in processing and interpreting extensive clinical documentation.
   * This benchmark consists of 20 detailed fictional patient cases across various diseases, with each case containing between 5,090 to 6,754 words.
   * The LongHealth benchmark challenges LLMs with 400 multiple-choice questions categorized into information extraction, negation, and sorting, providing a robust assessment tool for LLMs in the healthcare context.
   * The goal of this specific technique is to address the issues of long context windows and distributed relevant information.
   * Link to repo: https://github.com/kbressem/LongHealth



## Data Augmentation
* This is a technique often used in text generation prompting, fine-tuning and pre-training as well as evaluation of Generative AI outputs.
* Excellent review article I am referencing below: https://medium.com/@nielsencastelodamascenodantas/data-augmentation-in-nlp-6264107a28fe
* These include but are not limited to:
  * 1. Synonym Replacement (SR)
       * Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.

  * 2. Random Insertion (RI)
       * Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.

  * 3. Random Swap (RS)
       * Randomly choose two words in the sentence and swap their positions. Do this n times.

  * 4. Random Deletion (RD)
       * Randomly remove each word in the sentence with probability p.
* Other methods:
  * Psuedo Labeling
    * Pseudo-labeling involves using a trained model to generate labels for unlabeled data.
    * The model makes predictions on this unlabeled data, and these predictions are used as pseudo-labels to train the model again.
    * This allows leveraging large amounts of unlabeled data.
    * Pseudo-labeling is a semi-supervised learning technique that combines labeled and unlabeled data to improve model performance.

  * Generative Pseudo Labeling
    * GPL generally consists of 3 data preparation steps and one fine-tuning step. The three steps are:
      * 1. Query generation, creating queries from passages.
      * 2. Negative mining, retrieving similar passages that do not match (negatives).
      * 3. Pseudo labeling, using a cross-encoder model to assign similarity scores to pairs.
           * Excellent resource from Pinecone on this technique: https://www.pinecone.io/learn/series/nlp/gpl/

  * Active Learning
    * This involves training a model by actively selecting the most usefule contextual samples to be labeled by an expert.
    * The model is then trained on the samples labeled by the expert.
    * New most informative samples are continuously selected for labeling and training by the expert.
    * This minimizes the need for large amounts of labeled data.


