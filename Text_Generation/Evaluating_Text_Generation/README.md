# Evaluating Text Generation
* A repository that contains information on various methods for evaluation of synthetic text generation.


## General Text Generation Evaluation 
* To evaluate the quality of the synthetic notes generated by the LLMs, I often consider the following criteria:
    * **Coherence**: text should be logically structured and flow smoothly from one section to another.
    * **Relevance**: text should focus on the primary concepts and if medical text it should focus on the diagnosis and the specified symptoms, without irrelevant information.
    * **Domain accuracy**: information provided in the text should be domain accurate and follow standard domain practices (e.g. medical, law, media, etc.)
    * **Readability**: text generationed should be easy to read and understand, with clear and concise language.
    * **Completeness**: text should cover all essential sections of a typical domain specific text document (e.g. medical domain it would be hospital history and physical examination, legal it would be a legal brief, etc.)

### Techniques to Measure these criteria
    * 1) **Manual evaluation**
      * Have a panel of domain professionals or subject matter experts review a sample of the generated text and rate them based on the above criteria.
    * 2) **Automated evaluation**
      * Use NLP techniques to evaluate the generated text based on predefined metrics, such as:
        * a. **Coherence**: Measure the semantic similarity between adjacent sentences or sections using embeddings or language models.
          * There is also a technique that leverages the LongFormer Transformer model that can be finetuned to classify text as "coherent" vs. "non-coherent"
            * Model card: https://huggingface.co/lenguist/longformer-coherence-synthetic-classifier
        * b. **Relevance**: Calculate the cosine similarity between the text generated and a given set of entities, terms or queries (e.g. medical domain it would be the  diagnosis/symptom terms). 
        * c. **Domain accuracy**: Use named entity recognition (NER) and relation extraction to identify and validate domain speciic concepts and their relationships within the text. 
        * d. **Readability**: Calculate readability scores (e.g., Flesch-Kincaid, SMOG) to assess the complexity of the language used.
          * There are many readability metrics that exist, most of them are covered in this python library: https://pypi.org/project/py-readability-metrics/
        * e. **Completeness**: Depending on the domain, check for the presence of essential sections of the text (e.g. for medical do main check for chief complaint, history of present illness, review of systems) using text classification models.

### Textual Entailment
* This technique is very commonly used and this is why I have put it within its own category.
* Measuring textual coherence and Generative AI outputs is easy for a human to do (within reason) and more recently has become dominated by using LLM based frameworks to measure if text is coherent and not hallucination.
* “LLM graded evaluations” may help in evaluating text generation to a certain extent, however it is not really a consistent approach.
  * This is because we are using the same LLMs which are by design made to hallucinate outputs.
* This is why we need to look at non-LLM methods of evaluating Generative AI outputs. One such technique is **NLI(Natural Language Inference)**
* Textual entailment recognition is the task of deciding, given two text fragments, whether:
  * The meaning of one text is entailed (can be inferred) from another text (Dagan and Glickman 2004).
  * This task captures generically a broad range of inferences that are relevant for multiple applications.
  * For example, a question answering (QA) system has to identify texts that entail the expected answer. Given the question ‘Who is Kurt Cobain's widow?’ the text ‘There’s a song for Kurt Cobain on nearly every Hole album, spreading Kurt Cobain's influence across the span of Courtney Love’s career. The first is often considered to be ‘Doll Parts’, a track that arrived as the first single to be released from their second album, Live Through This’ which was written about her late husband", entails the expected answer ‘Courtney Love is Kurt Cobain’s widow’.

#### Recognizing textual entailment: Rational, evaluation and approaches
* Book by Dagan et al. 2013: https://link.springer.com/book/10.1007/978-3-031-02151-0
* An NLI model is typically a traditional **multi-classification** deep learning model created through a supervised approach of training with labelled datasets.
* The NLI training or the task formulation follows certain principles to answer: **“Does the premise justify an inference to the hypothesis?”**
* Those principles are as follows (Deb, 2024)[https://medium.com/@rajib76.gcp/nli-natural-language-inferencing-8f231f23800e]
  * 1. Derive the inferential relationship through common sense reason rather that strict logical reasoning
  * 2. Use short and direct inferencing steps, instead of a deductive chain
  * 3. Apply linguistic variation while formulating the tasks

* NLI is thus an approach to determine whether a **hypothesis** (the response)entails(entailment), **contradicts(contradiction)** or is **neutral(neutral)** to a given premise(the context).
* There are various NLI approaches using datasets and models pre-trained and fine-tuned on these tasks:
  * 1. SNLI
       * SNLI or Stanford Natural Language Inference corpus contains around 550K hypothesis/premise pairs. All the premises are image captions from Flickr30K corpus. All the hypothesis were crowd sourced through crowd workers
  * 2. ***MultiNLI***
       * This is based off a famous dataset from NYU: https://huggingface.co/datasets/nyu-mll/multi_nli
       * A model that has been specifically trained on this dataset is the `roberta-large-mnli` from Meta/Facebook: https://huggingface.co/FacebookAI/roberta-large-mnli
       * How this works:
         * 1. RoBERTa (specifically "roberta-large-mnli"), is pre-trained on the MultiNLI (Multi-Genre Natural Language Inference) dataset. This model is very good at **understanding relationships between sentences.**
         * 2. Coherence Metric - its important to incorporate this metric which uses the "entailment" score as a proxy for coherence. In NLI tasks, **entailment indicates that the second sentence logically follows from the first, which is thus interpreted as a measure of coherence.**
         * 3. Sentence-level Analysis: By evaluating pairs of consecutive sentences using the NLI approach, we are thus able to assess local coherence throughout a document of any length. 
         * 4. Flexibility - you don't have to use `roberta-large-mnl`, this is just a popular model. You can fine-tune pretty much any tranformer model using an NLI dataset. 
         * 5. Output - The evaluation output of these models will return a single float value representing the overall coherence of the text you are evaluating. 
      * This evaluation approach provides a **quantitative measure** of **text generation quality**, focusing on the **logical flow and consistency between consecutive sentences**.
           * It's an excellent use case for natural language inference models to assess document/text coherence, which is particularly useful for **evaluating synthetic text generation.**


  * 3. ANLI
       * ANLI or adversial natural language inference has more than 162K hypothesis/premise pairs. The premises are sourced from diverse sources. The hypothesis were crowd sources and written with the goal of fooling the SOTA models, hence the name adversial.
       * A good example of a model fine-tuned for this task is: `MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli`
         * model card: https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli

* A good list of NLI NLP resources: https://medium.com/@moscantonio/list/nli-nlp-272baac1156a

#### MedNLI
* MedNLI is one of the most popular textual entailment datasets out there.
* It is easily accessible through PhysioNet.
* Background
  * MednLI is a dataset annotated by doctors, performing a natural language inference tasks, grounded in the medical history of patients.
  * The authors present strategies to:
       * 1) leverage transfer learning using datasets from the open domain, (e.g. SNLI), and
       * 2) incorporate domain knowledge from external data and lexical sources (e.g. medical terminologies).
  * Their results demonstrate performance gains using both strategies.
 
* MedNLI Resources:
  * github: https://jgc128.github.io/mednli/
  * Hugging Face dataset card: https://huggingface.co/datasets/bigbio/mednli

#### Incorporating Domain Knowledge into Medical NLI using Knowledge Graphs
* This is a 2019 paper by Sharma et al.
  * arxiv paper: https://arxiv.org/abs/1909.00160
* The authors explored how to incorporate structured domain knowledge, using an ontology/knowledge graph (the UMLS), for the Medical NLI task.       * Specifically, they experimented with fusing embeddings obtained from knowledge graphs with the state-of-the-art approaches for NLI task (ESIM model).
  * They also experimented with fusing the domain-specific sentiment information for the task. Experiments conducted on MedNLI dataset clearly show that this strategy improves the baseline BioELMo architecture for the Medical NLI task.


### Other Techniques to Measure Text Generation
* 1) **CONNER, a COmpreheNsive kNowledge Evaluation fRamework**
      * This is a novel technique proposed by Chen at al. in a 2023 arxiv paper --> link to paper: https://arxiv.org/abs/2310.07289
      * Their technique is designed to systematically and automatically evaluate generated knowledge from 6 important perspectives including:
          * 1. Factuality
          * 2. Relevance
          * 3. Coherence
          * 4. Informativeness
          * 5. Helpfulness
          * 6. Validity

      * Link to github: https://github.com/ChanLiang/CONNER

* 2) **CoUDA: Coherence Evaluation via Unified Data Augmentation**
      * This is a novel technique by Zhu et al. proposed in a 2024 arxiv paper: https://arxiv.org/abs/2404.00681.
      * Due to the paucity of annotated data, data augmentation is commonly used for training coherence evaluation models especially for evaluating synthetic text generation. 
      * The authors state that previous augmentations for this task primarily rely on heuristic rules, lacking designing criteria as guidance. 
      * In this paper, the authors propose a data augmentation framework entitled "COUDA".
        * The technique breaks down discourse coherence into global and local aspects, and designs augmentation strategies for both aspects, respectively. 
        * For local coherence they propose a novel generative strategy for constructing augmentation samples, which involves post-pretraining a generative model and applying two controlling mechanisms to control the difficulty of generated samples. 
          * During inference, COUDA also jointly evaluates both global and local aspects to comprehensively assess the overall coherence of a discourse. 
          * The authors conducted extensive experiments in coherence evaluation to show that with only 233M parameters, COUDA achieves state-ofthe-art performance in both pointwise scoring and pairwise ranking tasks, even surpassing recent GPT-3.5 and GPT-4 based metrics.
         
* 3) **Shuffle Test**
     * Laban et al. in a 2021 paper critically examined the usefulness of the "Shuffle Test" to measure text coherence and developed a novel technique to overcome its limitations called the "K-Block Shuffle Test". 
       * arxiv paper link: https://arxiv.org/abs/2107.03448
       * github link: https://github.com/tingofurro/shuffle_test
     * The Shuffle Test was first introduced by Barzilay and Lapata in 2008.
         * It is the most common task for coherence model evaluation.
         * The task is a binary classification problem, in which a model must discriminate between a document and a shuffled document, obtained by randomly shuffling the order of sentences in the document. The most common dataset for evaluation is a set of articles from the Wall Street Journal, but obviously this is adopted based on the specific domain of interest.
     * The Insertion Test is when a single sentence from a document is removed, and the model must predict the sentence position.
         * Typically, models assign a score to each possible position, and predict the one with highest score.
         * A major limitation of the Insertion Test is that model accuracies are low, often in the 10-20% range (Elsner and Charniak, 2011).
      
     * The Sentence Ordering Task is when a model is given an randomly ordered sentence set, and must produce the correct ordering of sentences.
         * This task is often restricted to generative models, as it is prohibitively expensive to score all combinations to extract a best-scoring order (Logeswaran et al., 2018).
    * The authors introduce the novel **K-block Shuffle Test** to improve upon the limitations of the Shuffle test and its iterations.
        * In the standard Shuffle Test, text is divided into sentences and shuffled, with a unit of one sentence.
        * However, in the k-Block Shuffle Test, sentences are grouped in contiguous blocks of k sentences (resembling paragraphs), and the blocks are shuffled, maintaining sentence order in each block.
        * Within a block, sentences remain locally coherent, and as block size increases, the fraction of correct sentence transitions increases, while potentially incoherent transitions decrease.


  




## Medical Domain
1. Medcat
   * While this is not a direct evaluation tool, it is often used in tandem with text generation models.
   * Link to github: https://github.com/CogStack/MedCAT
  
2. medAlpaca: Finetuned Large Language Models for Medical Question Answering
   * These are an advanced suite of large language models specifically fine-tuned for medical question-answering and dialogue applications.
   * While the main intention of these models is for buiding medical chatbots, you could extend use cases to evaluating medical text generation.
   * Link to repo: https://github.com/kbressem/medAlpaca
  
3. RaTEScore
   * This is a novel, entity-aware metric to assess the quality of medical reports generated by AI models.
   * It emphasizes common and significant medical entities such as diagnostic outcomes and anatomical details, and is robust against complex medical synonyms and sensitive to negation expressions.
   * The evaluations demonstrate that RaTEScore aligns more closely with human preference than existing metrics.
   * While the primary use case is for Radiology Report Generation Evaluation, the concepts can be fine-tuned and extended to other medical report use cases.
   * The authors have released a research paper that explains some of the major issues with current text generation evaluation metrics, see the authors links here:
     * Original 2024 arxiv paper here: https://arxiv.org/abs/2406.16845
     * Authors research homepage: https://angelakeke.github.io/RaTEScore/
     * RaTEScore github: https://github.com/MAGIC-AI4Med/RaTEScore
     * An excellent Figure from the original paper from Zhao et al. showing the limitations of current metrics for evaluation of medical text generation:
       ![image](https://github.com/user-attachments/assets/39ba7de7-a7a7-4460-b306-9d2eeccc6e31)


4. PMC-LLaMA: Towards Building Open-source Language Models for Medicine
   * Original paper by Wu et al. 2023: https://arxiv.org/abs/2304.14454
   * PMC-LLaMA github: https://github.com/chaoyi-wu/PMC-LLaMA
  
5. LongHealth: A Question Answering Benchmark with Long Clinical Documents
   * LongHealth is a comprehensive benchmark designed to evaluate the capabilities of large language models (LLMs) in processing and interpreting extensive clinical documentation.
   * This benchmark consists of 20 detailed fictional patient cases across various diseases, with each case containing between 5,090 to 6,754 words.
   * The LongHealth benchmark challenges LLMs with 400 multiple-choice questions categorized into information extraction, negation, and sorting, providing a robust assessment tool for LLMs in the healthcare context.
   * The goal of this specific technique is to address the issues of long context windows and distributed relevant information.
   * Link to repo: https://github.com/kbressem/LongHealth


### MedSyn
* This is a recent paper published on arxiv in August 2024 by Kumichev et al.
  * Link to paper: https://arxiv.org/abs/2408.02056
  * github: https://github.com/milteam/MedSyn
* The purpose of the authors work was to introduce MedSyn, a novel medical text generation framework that integrates LLMs with a Medical Knowledge Graph (MKG).
  * They used a MKG to sample prior medical information for the prompt and generate synthetic clinical notes with GPT-4 and fine-tuned LLaMA models.
  * They assessed the benefit of synthetic data through application in an ICD code prediction task.
  * Results of their research indicates:
    * **synthetic data can increase the classification accuracy of vital and challenging codes by up to 17.8% compared to settings without synthetic data.**
    * ** In addition, to provide new data for further research in the healthcare domain, the authors produced the largest open-source synthetic dataset of clinical notes for the Russian language, comprising over 41k samples covering 219 ICD-10 codes.**
* **This framework could be useful for synthetic medical note generation in any language but also as a potential framework for evaluating and parsing real-world medical notes and synthetic generated medical notes.**

* Framework from the Kumichev et al. paper:
![image](https://github.com/user-attachments/assets/b17ef286-6e37-45d1-8bfa-d7df3f184f2c)


### Additional Techniques related to MedSyn
1. **BioLORD**
   * BioLORD introduces semantic context to enrich biomedical representations.
   * The authors pair concepts with texts, which can be either their definitions or descriptions.
         * The definitions are sourced directly from the UMLS ontology, but these are available for only ~5% of the concepts.
         * To extend the semantic coverage for the other concepts, the authors generate simple descriptions based on the concept-to-concept relations present in the UMLS knowledge graph.
         * example: `Template: “[more-generic-concept] which [has-relationship-with] [related-concept]”​`
            * The concepts and their respective contexts are then used as positive pairs in a contrastive learning setting, where positive pairs are attracted in the embedding space, and negative ones are repelled.
            * A detail: one concept may have different names. Thus, there may be more than one pair referring to the same concept. Ideally, these will all end up close in the trained embedding space.
   * Figure 1 from the original paper:
![image](https://github.com/user-attachments/assets/c8675e51-6b73-40bf-9d57-bf15bc67e7f1)

   * BioLORD Resources
      * (Remy et al. 2022 - BioLORD: Learning Ontological Representations from Definitions (for Biomedical Concepts and their Textual Descriptions)[https://arxiv.org/abs/2210.11892]
      * (Remy et al. 2023 - BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical Knowledge Graph Insights)[https://arxiv.org/abs/2311.16075]
      * (BioLORD: Contrastive learning for biomedical concepts)[https://hossboll.medium.com/biolord-contrastive-learning-for-biomedical-concepts-32c453f7a830]
      * Hugging Face - BioLORD Dataset: https://huggingface.co/datasets/FremyCompany/BioLORD-Dataset
      * Hugging Face - BioLORD Sentence Transformers model: https://huggingface.co/FremyCompany/BioLORD-2023-S





## Data Augmentation
* This is a technique often used in text generation prompting, fine-tuning and pre-training as well as evaluation of Generative AI outputs.
* Excellent review article I am referencing below: https://medium.com/@nielsencastelodamascenodantas/data-augmentation-in-nlp-6264107a28fe
* These include but are not limited to:
  * 1. Synonym Replacement (SR)
       * Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.

  * 2. Random Insertion (RI)
       * Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.

  * 3. Random Swap (RS)
       * Randomly choose two words in the sentence and swap their positions. Do this n times.

  * 4. Random Deletion (RD)
       * Randomly remove each word in the sentence with probability p.
* Other methods:
  * Psuedo Labeling
    * Pseudo-labeling involves using a trained model to generate labels for unlabeled data.
    * The model makes predictions on this unlabeled data, and these predictions are used as pseudo-labels to train the model again.
    * This allows leveraging large amounts of unlabeled data.
    * Pseudo-labeling is a semi-supervised learning technique that combines labeled and unlabeled data to improve model performance.

  * Generative Pseudo Labeling
    * GPL generally consists of 3 data preparation steps and one fine-tuning step. The three steps are:
      * 1. Query generation, creating queries from passages.
      * 2. Negative mining, retrieving similar passages that do not match (negatives).
      * 3. Pseudo labeling, using a cross-encoder model to assign similarity scores to pairs.
           * Excellent resource from Pinecone on this technique: https://www.pinecone.io/learn/series/nlp/gpl/

  * Active Learning
    * This involves training a model by actively selecting the most usefule contextual samples to be labeled by an expert.
    * The model is then trained on the samples labeled by the expert.
    * New most informative samples are continuously selected for labeling and training by the expert.
    * This minimizes the need for large amounts of labeled data.


# Unit Testing Evaluations
* Unit testing is common in Data Science, Machine Learning, and Software Engineering.
* There are many popular and useful frameworks to evaluate text generation by LLMs and transformer models. You can certainly use open source out of the box frameworks with ease.

## **Pytest** is one of the most popular and commonly used testing frameworks and can be utilized for evaluating LLM outputs of all types.
  1. You can test LLM output consistency.
  2. Compare performance of prompts and models.
     * This is an excellent blog post about this approach: (Pytest is All You Need)[https://arjunbansal.substack.com/p/pytest-is-all-you-need]

## Llama-2 Unit Testing
* There are some fine tuned LLMs such as llama-2 on huggingface that can be used specifically for unit testing.
* `h2oai/llama2-0b-unit-test` is an example here: https://huggingface.co/h2oai/llama2-0b-unit-test

## Meta's TestGen LLM
* TestGen-LLM is able to evaluate unit tests and identify immediate areas for improvements. It is able to do this through its understanding of common testing patterns on which it has been pre-trained.
* However, generating unit tests alone is insufficient for proper code coverage. Meta also implemented safeguards within TestGen-LLM to ensure the effectiveness of unit tests it generates. These safeguards, referred to as **filters**, act as a quality control mechanism. They eliminate suggestions that:
   * 1. wouldn't compile
   * 2. fail consistently, or
   * 3. fail to actually improve code coverage (suggestions that are already covered by other tests).
    
* These are a few excellent resources about TestGen LLM
  * Original arxiv paper: (Automated Unit Test Improvement using Large Language Models at Meta)[https://arxiv.org/abs/2402.09171]
  * (How to Use AI to Automate Unit Testing with TestGen-LLM and Cover-Agent)[https://www.freecodecamp.org/news/automated-unit-testing-with-testgen-llm-and-cover-agent/#heading-metas-testgen-llm]
  * (Meta's new LLM-based test generator is a sneak peek to the future of development)[https://read.engineerscodex.com/p/metas-new-llm-based-test-generator]
