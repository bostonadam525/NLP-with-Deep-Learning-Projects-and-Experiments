{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation Hugging Face Pipeline\n",
        "* Notebook by Adam Lang\n",
        "* Date: 12/3/2024\n",
        "\n",
        "# Overview\n",
        "* In this notebook I will demo how to implement a simple text generation pipeline in hugging face."
      ],
      "metadata": {
        "id": "EYvDfUltUo5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "sBoGs4cnUz9V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOc8vs5iUl5E",
        "outputId": "6a4231ed-6dc4-4a98-b404-98e10bf3c45e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.2\n",
            "    Uninstalling transformers-4.46.2:\n",
            "      Successfully uninstalled transformers-4.46.2\n",
            "Successfully installed transformers-4.46.3\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.6)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers #upgrades\n",
        "!pip install -U sentencepiece #upgrades\n",
        "!pip install -U sacremoses #upgrades"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## imports - transformers\n",
        "from transformers import pipeline\n",
        "from transformers import set_seed\n",
        "set_seed(42) #set seed for consistency\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "FESAE5zgU343"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Text Generation Pipeline\n",
        "* The default text generator model is: `openai-community/gpt2 and revision 607a30d`\n",
        "  * model card: https://huggingface.co/openai-community/gpt2"
      ],
      "metadata": {
        "id": "ePnSWtHbVKpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## lets use some dummy text data as an example\n",
        "text = \"\"\"\n",
        "Dear Amazon, last week I ordered a new pair of alpine skis\n",
        "from your online store in Seattle. Unfortunately when I opened\n",
        "the package, I discovered that I had accidentally been sent a Snowboard instead!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rQx3vaXgVchN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## generator setup\n",
        "generator = pipeline(\"text-generation\")\n",
        "\n",
        "# response\n",
        "response = \"I am so sorry to hear that your order was mixed up\"\n",
        "\n",
        "# prompt --> replace new line chars with `.replace`\n",
        "prompt = \"user: \" + text.replace(\"\\n\", \" \") + \" Customer Service Response: \" + response\n",
        "\n",
        "## get outputs from text generator\n",
        "outputs= generator(prompt,\n",
        "                   max_length=128, #change based on use case\n",
        "                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDIV9KnSVF0d",
        "outputId": "3f55a629-e0cd-458a-d16e-0ee97e14e5b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## outputs\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmXAXqg1WsQz",
        "outputId": "875a1d30-c8e4-4251-a6f0-55a2445cc9b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'user:  Dear Amazon, last week I ordered a new pair of alpine skis from your online store in Seattle. Unfortunately when I opened the package, I discovered that I had accidentally been sent a Snowboard instead!  Customer Service Response: I am so sorry to hear that your order was mixed up with a snowboard shipment!  I am so sorry I purchased a pair of Alpaca in Amazon and would love to review both of them in the future to check their compatibility. Thank you for visiting the store and I look forward to seeing them next time. Thanks for your interest in my order!!!! I would appreciate a little'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "* We can see that GPT-2 is hallucinating its answer as it doesn't have much context and is clearly regurgitating content from the internet corpus it was trained on.\n",
        "* Let's see if we can improve this!"
      ],
      "metadata": {
        "id": "Adtf_w7PXvSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try a simple prompt about food."
      ],
      "metadata": {
        "id": "XRt01Gpbbz_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## generator setup\n",
        "generator = pipeline(\"text-generation\")\n",
        "\n",
        "# response\n",
        "#response = \"I am so sorry to hear that your order was mixed up\"\n",
        "\n",
        "# prompt --> replace new line chars with `.replace`\n",
        "#prompt = \"user: \" + text.replace(\"\\n\", \" \") + \" Customer Service Response: \" + response\n",
        "\n",
        "## new prompt\n",
        "prompt = \"\"\"\n",
        "Tell me how to cook pancakes.\n",
        "\"\"\"\n",
        "## get outputs from text generator\n",
        "outputs= generator(prompt,\n",
        "                   max_length=128, #change based on use case\n",
        "                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6bfaf2d-981d-4ec9-9486-434e3d5bbd4d",
        "id": "saCr-CfbYCfA"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "Markdown(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "JbijwH_EYQ1x",
        "outputId": "75dcc3ef-4014-469b-fc75-af51dd4f255b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nTell me how to cook pancakes.\n\nA couple other people are coming over.\n\nDo I sound like I'm some kind of dumb dork?\n\nIt's not funny to be the dumb dork.\n\nI guess I don't have to worry about that too much.\n\nWhat if I just don't cook pancakes, do I?\n\nIt's really hard sometimes.\n\nI feel like I can't even have an egg in my pancake.\n\nMy heart rate is so high, I feel like I are going to explode.\n\nI really don't have a problem with"
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "* The default `gpt-2` model wrote us a poem about pancakes, interesting.\n",
        "* Let's see if we can improve this with a specific model in the pipeline."
      ],
      "metadata": {
        "id": "CqkEI-QyaF9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's try using an open source Text Generation model in the pipeline\n",
        "* We will try a smaller LLM from hugging face.\n",
        "* This is the model: `openai-community/gpt2-large`\n",
        "  * model card: https://huggingface.co/openai-community/gpt2-large"
      ],
      "metadata": {
        "id": "KCUcfEZBahjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## generator setup\n",
        "generator = pipeline(\"text-generation\",\n",
        "                     model=\"openai-community/gpt2-large\")\n",
        "\n",
        "# response\n",
        "response = \"I am so sorry to hear that your order was mixed up\"\n",
        "\n",
        "# prompt --> replace new line chars with `.replace`\n",
        "#prompt = \"user: \" + text.replace(\"\\n\", \" \") + \" Customer Service Response: \" + response\n",
        "\n",
        "## new prompt\n",
        "prompt = \"\"\"\n",
        "Tell me how to cook pancakes.\n",
        "\"\"\"\n",
        "## get outputs from text generator\n",
        "outputs= generator(prompt,\n",
        "                   max_length=128, #change based on use case\n",
        "                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e497d5-91b0-4299-f37c-425dae0b6fed",
        "id": "zjUQYojMbGYv"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "Markdown(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "7f10a82c-d62e-4e46-871f-0b76641e59c3",
        "id": "CXdhIjyUbmrn"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nTell me how to cook pancakes.\n\nPancakes are often made from a combination of flour, eggs, and milk. One of the most important steps in making your own pancake is making sure you get all your flour, eggs, and milk together in a single piece or you will get a wet ball of flour.\n\nTo do this, you must first create flour and milk in a blender. Some people use a food processor, but I prefer to use a blender because the flour will separate easier and the batter will become thicker, which makes it easier to mix later.\n\nWhat are your tips for the proper"
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "* We now have an output that aligns more with what we actually prompted which was generated text to tell me how to make pancakes.\n",
        "* Obviously this demonstrates how the pipelines run out of the box but also how there is a lot more that we need to do to make the text generation more specific and actually answer our question or prompt."
      ],
      "metadata": {
        "id": "pkN9RRLadmNL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OIIXKwFDbpEp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}